[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gut microbiome analysis for CRC detection: from raw sequences to machine learning models",
    "section": "",
    "text": "Preface\nThis book is a result of the author’s reflection of his learning of the field of bioinformatics. This book is created to bundle up all relevant learning materials and tutorials for gut microbiome analysis with a particular focus on early CRC detection.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "chapters/16S/16S_processing_intro.html",
    "href": "chapters/16S/16S_processing_intro.html",
    "title": "16S rRNA Sequence Processing",
    "section": "",
    "text": "Steps\nIn this part of the book, we will dive into processing steps for 16S rRNA raw reads to obtain features such as Operational Taxonomic Units (OTUs) and Amplicon Sequence Variants (ASV). This steps will provide a detailed explanation and demonstrate the usage of relevant tools.\nWe will basically follow the process depicted in the below diagram.\nflowchart LR\n    X(Reference DB) --&gt;D(Taxonomic Profiling)\n    A(FASTQ) --&gt; B(Quality Control)\n    B --&gt; C(Denoising)\n    C --&gt; D\n    D --&gt; F(Taxonomic Profile)",
    "crumbs": [
      "16S rRNA Sequence Processing"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter0.html",
    "href": "chapters/16S/16S_chapter0.html",
    "title": "Preparing reference database",
    "section": "",
    "text": "Installing RESCRIPt plugin\nWe will first setup our reference database in QIIME2. For that we are going to use SILVA database (Pruesse et al. 2007; Quast et al. 2012) To simplify this process, we will use an excelent plugin that is Rescript (Robeson et al. 2021). This plugin provides an already built pipeline to download and process different reference databases (read more here).\nWe will use the following command to install the plugin",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing reference database</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter0.html#installing-rescript-plugin",
    "href": "chapters/16S/16S_chapter0.html#installing-rescript-plugin",
    "title": "Preparing reference database",
    "section": "",
    "text": "pip install git+https://github.com/bokulich-lab/RESCRIPt.git",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing reference database</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter0.html#preparing-silva-database-as-reference-database",
    "href": "chapters/16S/16S_chapter0.html#preparing-silva-database-as-reference-database",
    "title": "Preparing reference database",
    "section": "Preparing SILVA database as reference database",
    "text": "Preparing SILVA database as reference database\nWe will follow steps provided here to download and build SILVA database for use in QIIME.\n\nStep-1: Download SILVA\nqiime rescript get-silva-data \\\n    --p-version '138.1' \\\n    --p-target 'SSURef_NR99' \\\n    --o-silva-sequences silva-138.1-ssu-nr99-rna-seqs.qza \\\n    --o-silva-taxonomy silva-138.1-ssu-nr99-tax.qza\n\n\n\n\n\n\nRESCRIPt error\n\n\n\nRESCRIPt plugin works well for some QIIME versions. In case if you encounter the problem of cannot import name 'DNASequence' from 'q2_types.genome_data' then refer to this page for detailed instructions to resolve the issue. The same steps are also provided below.\nWe will create an additional conda environment and install required packages to allow rescript to function properly.\nconda install -c conda-forge -c bioconda -c qiime2 \\\n-c https://packages.qiime2.org/qiime2/2023.9/shotgun/released/  \\\n-c defaults   xmltodict 'q2-types-genomics&gt;2023.5' ncbi-datasets-pylib\n\npip install git+https://github.com/bokulich-lab/RESCRIPt.git\n\nqiime dev refresh-cache\n\nqiime rescript --help\n\n\n\n\nStep-2: Converting rna sequences to dna sequences\nThe resultant sequences from the above step are of ‘RNASequences’ data type. To ensure a smooth downstream analysis, we will convert data type to ‘DNASequences’ using the following command.\nqiime rescript reverse-transcribe \\\n    --i-rna-sequences silva-138.1-ssu-nr99-rna-seqs.qza \n    --o-dna-sequences silva-138.1-ssu-nr99-seqs.qza\nThe resultant qiime artifact silva-138.1-ssu-nr99-seqs.qza now can be used to train a classifier. Additional steps could also be integrated in the process before building the classifier. Such as cutting low quality sequences, filtering based on length, etc.\nThe link provides some of those examples and command to do that using RESCRIPt plugin.",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing reference database</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter0.html#building-a-taxonomy-classifer",
    "href": "chapters/16S/16S_chapter0.html#building-a-taxonomy-classifer",
    "title": "Preparing reference database",
    "section": "Building a taxonomy classifer",
    "text": "Building a taxonomy classifer\nNow we will move to train a taxonomy classifier which we are going to use later in our analysis to assign taxonomy labels to sequence data. Before doing that we will select V4 region from the SILVA database and use that extracted database for training our classifier.\nThis step of extracting V4 regions has been found to improve the performance.\nExtract V4 region\nqiime feature-classifier extract-reads \\\n    --i-sequences silva-138.1-ssu-nr99-seqs.qza \\\n    --p-f-primer GTGYCAGCMGCCGCGGTAA \\\n    --p-r-primer GGACTACNVGGGTWTCTAAT \\\n    --p-n-jobs 2 \\\n    --p-read-orientation 'forward' \\\n    --o-reads silva-138.1-ssu-nr99-seqs-515f-806r.qza\nTrain the classifier\nqiime feature-classifier fit-classifier-naive-bayes \\\n  --i-reference-reads silva-138.1-ssu-nr99-seqs-515f-806r.qza \\\n  --i-reference-taxonomy silva-138.1-ssu-nr99-tax.qza \\\n  --o-classifier silva_classifier.qza",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Preparing reference database</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter1.html",
    "href": "chapters/16S/16S_chapter1.html",
    "title": "Quality control",
    "section": "",
    "text": "Generating quality report using fastqc\nThe first step in the process is to learn about the quality of sequence reads. As Fastq files contains a quality score per base which indicates the level of accuracy for a particular base read. Our goal in the first step is to check is there something abnormal with the quality scores, e.g., do quality degrades after a particular read position.\nThankfully, we have tools which can automate this lookup process for us. The first tool is Fastqc which scans each sequence read file and then generate a report per file, providing essential information on many quality aspects of sequence data.\nNow imagine going through each generated file to come up with some sort of understanding of the overall quality of datasets. It could be an exhausting task. Thanks again to the bioinformatics field, we have another tool Multiqc which summarizes multiple report files into a single report summary which makes it easier to determine the quality of sequence reads.\nThe fastqc command has the following syntax\nHere, -o flag specifies output directory, and -t flag specifies number of threads to process the files.",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter1.html#generating-quality-report-using-fastqc",
    "href": "chapters/16S/16S_chapter1.html#generating-quality-report-using-fastqc",
    "title": "Quality control",
    "section": "",
    "text": "fastqc  -o ./fastqc_results -t 10 ./raw_data/*.fastq.gz",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter1.html#summarizing-all-quality-reports-using-multiqc",
    "href": "chapters/16S/16S_chapter1.html#summarizing-all-quality-reports-using-multiqc",
    "title": "Quality control",
    "section": "Summarizing all quality reports using multiqc",
    "text": "Summarizing all quality reports using multiqc\nThe following command runs multiqc and generate a summary report of fastqc reports.\nmultiqc ./fastqc_results -o ./multiqc_results",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter1.html#applying-trimmomatic",
    "href": "chapters/16S/16S_chapter1.html#applying-trimmomatic",
    "title": "Quality control",
    "section": "Applying trimmomatic",
    "text": "Applying trimmomatic\nNow, we will use insights gained from multiqc report to determine some parameters such as minimum length of sequences. These parameters then we will pass in the trimmomatic command.\ntrimmomatic -PE ADenoma12-2799_S12_L001_R1_001.fastq.gz ADenoma12-2799_S12_L001_R2_001.fastq.gz -baseout Ademona1-2065_trimmed_S1.fastq.gz CROP:150\n\n\n\n\n\n\nTip\n\n\n\nSimilar command as above must be run for each paired (or single) end sequences file. To automate this, we can use a shell script.\n\n\n#!/usr/bin/env bash\n# author: pankaj chejara\n# Script to iterate over paired read sequences to apply trimmomatic\n\nsamples=()\n\nfor filename in ./raw_data/*.fastq.gz; do\n    base=$(basename \"$filename\" .fastq.gz)\n    nf=$(echo $base | sed -e 's/.......$//');\n    if ! [[ ${samples[@]} =~ $nf ]]\n    then\n      samples+=(\"$nf\");\n    fi\n  done\n\nfor sample in \"${samples[@]}\"; do\n  forward=\"./raw_data/${sample}_R1_001.fastq.gz\"\n  backward=\"./raw_data/${sample}_R2_001.fastq.gz\"\n  baseout=\"./trimm_outputs/${sample}.fastq.gz\"\n  trimmomatic PE -threads 20 $forward $backward -baseout $baseout CROP:200\"\ndone;",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter2.html",
    "href": "chapters/16S/16S_chapter2.html",
    "title": "Loading data in QIIME",
    "section": "",
    "text": "Create manifest file\nNow, we will import our dataset into QIIME2 for further processing. There are different data formats supported by QIIME2 when it comes to importing. So, having a basic knowledge about those formats and their corresponding procedures to import them would be helpful.\nMore information can be accessed here\nIn our previous step, we applied filtering using trimmomatic tool. This filtering step generated new sequence files with names with suffixes like 1P_L001.fastq.gz. We we will create a manifest file containing filepath of forward and backward sequence read files.\nTo automate the generation of manifest file, we will use the following shell script\nThis script will generate a manifest file which looks like this",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Loading data in QIIME</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter2.html#create-manifest-file",
    "href": "chapters/16S/16S_chapter2.html#create-manifest-file",
    "title": "Loading data in QIIME",
    "section": "",
    "text": "create_manifest.sh\n\n# author: Pankaj chejara\n# Script to create manifest file for qiime2\n\nsamples=()\n\nfor filename in ./raw_files/*.fastq.gz; do\n    base=$(basename \"$filename\" .fastq.gz)\n    nf=$(echo $base | sed -e 's/.......$//');\n    if ! [[ ${samples[@]} =~ $nf ]]\n    then\n      samples+=(\"$nf\");\n    fi\n  done\n\necho \"sample-id    forward-absolute-filepath    reverse-absolute-filepath\" &gt; manifest.csv\n\nfor sample in \"${samples[@]}\"; do\n  forward=\"$PWD/trimm_outputs/${sample}_1P.fastq.gz\"\n  backward=\"$PWD/trimm_outputs/${sample}_2P.fastq.gz\"\n  sampleid=$(echo $sample | sed -e 's/_.*//')\n  echo \"$sampleid    $forward    $backward\" &gt;&gt; manifest.csv\n  #echo \"$sampleid,$backward,backward\" &gt;&gt; manifest.csv\ndone;\n\n\n\n\n\n\n\n\n\n\nsample-id\nabsolute-filepath\ndirection\n\n\n\n\nADenoma12-2799\n/Users/pankaj/Documents/Metrosert/Public dataset/zakular2014/trimm_outputs/ADenoma12-2799_S12_L001_1P.fastq.gz\nforward\n\n\nADenoma12-2799\n/Users/pankaj/Documents/Metrosert/Public dataset/zakular2014/trimm_outputs/ADenoma12-2799_S12_L001_2P.fastq.gz\nbackward\n\n\nAdemona1-2065\n/Users/pankaj/Documents/Metrosert/Public dataset/zakular2014/trimm_outputs/Ademona1-2065_S1_L001_1P.fastq.gz\nforward\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDo not forget to change the directories in the script as per your environment. The script scans raw_data directory to fetch all sample ids and then generate manifest file with filepaths (of files obtained after trimmomatic).",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Loading data in QIIME</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter2.html#import-sequences-reads-in-qiime",
    "href": "chapters/16S/16S_chapter2.html#import-sequences-reads-in-qiime",
    "title": "Loading data in QIIME",
    "section": "Import sequences reads in QIIME",
    "text": "Import sequences reads in QIIME\nTo import our dataset, we will use the generated manifest file and run the following command.\nqiime tools import \\\n  --type 'SampleData[PairedEndSequencesWithQuality]' \\\n  --input-path manifest.csv \\\n  --input-format PairedEndFastqManifestPhred33V2 \\\n  --output-path paired-end-demux.qza\nOn successful execution of the above command, a new qiime artifact paired-end-demux.qza will be generated.",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Loading data in QIIME</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter3.html",
    "href": "chapters/16S/16S_chapter3.html",
    "title": "Removing primers",
    "section": "",
    "text": "This step will remove primers from sequence read data. For that we will use QIIME with cutadapt plugin.\nWe will use the information from (Zackular et al. 2014) on forward and reverse primer used in sequencing. The following command remove primers from our dataset.\nqiime cutadapt trim-paired \\\n--i-demultiplexed-sequences paired-end-demux.qza \\\n--p-front-f GTGYCAGCMGCCGCGGTAA \\\n--p-front-r GGACTACNVGGGTWTCTAAT \\\n--o-trimmed-sequences paired-end-demux-trimmed.qza\"\n\nReferences\n\n\n\n\nZackular, Joseph P, Mary AM Rogers, Mack T Ruffin IV, and Patrick D Schloss. 2014. “The Human Gut Microbiome as a Screening Tool for Colorectal Cancer.” Cancer Prevention Research 7 (11): 1112–21.",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Removing primers</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter4.html",
    "href": "chapters/16S/16S_chapter4.html",
    "title": "Denoising",
    "section": "",
    "text": "This step will denoise our sequence dataset and also remove chimera sequences. We will use dada2 (Callahan et al. 2016) for denoising.\nqiime dada2 denoise-paired \\\n  --i-demultiplexed-seqs demultiplexed-sequences.qza \\\n  --p-trunc-len-f 204 \\\n  --p-trim-left-r 1 \\\n  --p-trunc-len-r 205 \\\n  --o-representative-sequences asv-sequences-0.qza \\\n  --o-table feature-table-0.qza \\\n  --o-denoising-stats dada2-stats.qza\nWe will also generate a summary file to gain insights into the result of the command.\nqiime metadata tabulate \\\n  --m-input-file dada2-stats.qza \\\n  --o-visualization dada2-stats-summ.qzv\n\nReferences\n\n\n\n\nCallahan, Benjamin J, Paul J McMurdie, Michael J Rosen, Andrew W Han, Amy Jo A Johnson, and Susan P Holmes. 2016. “DADA2: High-Resolution Sample Inference from Illumina Amplicon Data.” Nature Methods 13 (7): 581–83.",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Denoising</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter5.html",
    "href": "chapters/16S/16S_chapter5.html",
    "title": "OTUs",
    "section": "",
    "text": "Merge forward and backward reads\nWe will now cluster sequences and assign taxonomy.\nAs the first step, we will merge forward and backward read sequences into one. The resultant sequences will be used for next steps.",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>OTUs</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter5.html#merge-forward-and-backward-reads",
    "href": "chapters/16S/16S_chapter5.html#merge-forward-and-backward-reads",
    "title": "OTUs",
    "section": "",
    "text": "qiime vsearch merge-pairs \\\n--i-demultiplexed-seqs paired-end-demux-trimmed.qza \\\n--o-merged-sequences demux-joined.qza \\\n--o-unmerged-sequences demux-unjoined.qza",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>OTUs</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter5.html#quality-control-of-merged-sequences",
    "href": "chapters/16S/16S_chapter5.html#quality-control-of-merged-sequences",
    "title": "OTUs",
    "section": "Quality control of merged sequences",
    "text": "Quality control of merged sequences\nNow we will use merged pairs and apply quality filtering.\nqiime quality-filter q-score \\\n–-i-demux demux-joined.qza \\\n--p-min-quality 20 \\\n–-o-filtered-sequences demux-joined-filtered.qza\n–-o-filter-stats demux-joined-filter-stats.qza",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>OTUs</span>"
    ]
  },
  {
    "objectID": "chapters/16S/16S_chapter5.html#denoise-using-deblur",
    "href": "chapters/16S/16S_chapter5.html#denoise-using-deblur",
    "title": "OTUs",
    "section": "Denoise using deblur",
    "text": "Denoise using deblur\nqiime deblur denoise-16S \\\n  --i-demultiplexed-seqs demux-joined-filtered.qza \\\n  --p-trim-length 200 \\\n  --p-sample-stats \\\n  --o-representative-sequences rep-seqs.qza \\\n  --o-table de_table.qza \\\n  --o-stats deblur-stats.qza",
    "crumbs": [
      "16S rRNA Sequence Processing",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>OTUs</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_processing_intro.html",
    "href": "chapters/Metagenomics/Meta_processing_intro.html",
    "title": "Metagenomics Sequence Processing",
    "section": "",
    "text": "Metagenomic raw sequences consist of strings composed of four letters: A, C, T, and G, representing the nucleotide bases Adenine, Cytosine, Thymine, and Guanine, respectively. These bases form the building blocks of DNA. Machines that sequence DNA extract these bases from a DNA sample and store them in files, commonly in formats such as FASTQ or FASTA.\nIn this tutorial, we will focus on the widely adopted FASTQ format. A FASTQ file typically contains essential information about the sample, the nucleotide sequences, and a quality score for each identified base. The quality score indicates the probability of correctly identifying a specific base, providing a valuable measure of sequencing accuracy.\nWe will use FASTQ files to prepare data for further analysis. The figure below illustrates the processing pipeline and the tools involved in this workflow.\n\n\n\n\n\nflowchart LR\n    A(FASTQ) --&gt; B(Quality Control)\n    B --&gt; I(Host DNA Removal)\n    I --&gt; C(Sequence Assembly)\n    C --&gt; D(Taxonomic Profiling)\n    C --&gt; E(Functional Annotation)\n    D --&gt; F(Taxonomic Profile)\n    E --&gt; G(Functional Pathways)\n\n\n\n\n\n\n\n\nQuality Control: This is the first step where we will assess the quality of raw sequences to identify low-quality reads and adapter contamination. Tools such as FASTQC, MULTIQC, and Fastp will help generate quality metrics like per-based quality scores and read length distribution. Trimming tools like Trimmomatic or Trim-glore then help clean the data.\nHuman DNA Removal: This step removes host DNA from metagenomic raw sequences to ensure that the downstream analysis focuses fully to microbial sequences. Tools such as bowtie2 and samtools will be used for this task. The reference human genome (e.g., GRCh38) will be downloaded and indexed before alignment.\nSequence Assembly: In this step, we will assemble short reads into longer sequences. Tools like SPAdes (ideal for small database) and MegaHit (optimized for large datasets) will be employed for assembly.\nTaxonomic Profiling: This step assigns operational taxonomic units (OTUs) to assembeled and quality controled raw sequences to determine the taxonomic composition of the microbiome. To execute this task, we will explore the usage of tools such as MetaPhlan2, BowTie2, and mOTUs.\nFunctional Annotation: While taxonomic profile identifies “who is there”, functional annotation determines “what they are doing”. It assigns functional roles and metabolic pathways present in provide metagenomic sequences. Tools like HUMAnN will be used for the task.",
    "crumbs": [
      "Metagenomics Sequence Processing"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter1.html",
    "href": "chapters/Metagenomics/Meta_chapter1.html",
    "title": "Quality control",
    "section": "",
    "text": "Trimming sequences to enhance quality\nWe will start by looking into quality scores of our sequences to do a quality check. To achieve this task, we use FASTQC program with raw sequencing data. FASTQC provides an easy-to-understand html report, facilitating an efficient quality check of fastq files.\nThe command below run the fastqc program for each fastq (or compressed fastq.gz) file and produces a report for each file in the output_directory.\nIn our case, we have multiple compressed fastq files with the file extension of .fastq.gz. We will use the expression *.fastq.gz to specify all those files for the processing. The following command run fastqc program for every file.\nThis execution results in the generation of a html and a zip file for each processed fastq file. The html file provides a report consisting of tables, figures about the quality check. The zip file contains all the figures, tables, and summaries. Figure fig-quality shows the distribution of quality scores for each position for a particular fastq file (SRR6915103_1).\nSuch report is generated for each fastq file and it can become cumbersome checking each of those files for quality check. To simplify this step and aggregate all such reports into a single one, we use MULTIQC program (Ewels et al. 2016).\nThe command below run multiqc program which generates a html report and a directory of images, tables, and summaries.\nThe below image shows a snapshot of multiqc report.\nThis task involves removing sequences due to low quality or due to other measures (e.g., shorter than a particular length).\nWe will use here trim-galore (Krueger 2015) to perform the trimming task. The program runs for each paired-eng sequence. Therefore, we will write a script to iterate over all the fastq files in the directory.\nThe following script provides that functionality.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter1.html#trimming-sequences-to-enhance-quality",
    "href": "chapters/Metagenomics/Meta_chapter1.html#trimming-sequences-to-enhance-quality",
    "title": "Quality control",
    "section": "",
    "text": "#!/bin/bash\n\n# Create output directory if it doesn't exist\nmkdir -p glore_output\n\n# Loop over all R1 files ending with _1.fastq.gz\nfor r1 in raw_data/*_1.fastq.gz; do\n\n  # Derive the corresponding R2 file by replacing _1 with _2\n  r2=\"raw_data/$(basename \"$r1\" _1.fastq.gz)_2.fastq.gz\"\n\n  # Run Trim Galore for each pair and store output in glore_output\n  trim_galore --nextera  --quality 20 --length 75 --paired \"$r1\" \"$r2\" --output_dir glore_output\ndone",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter1.html#references",
    "href": "chapters/Metagenomics/Meta_chapter1.html#references",
    "title": "Quality control",
    "section": "References",
    "text": "References\n\n\n\n\nEwels, Philip, Måns Magnusson, Sverker Lundin, and Max Käller. 2016. “MultiQC: Summarize Analysis Results for Multiple Tools and Samples in a Single Report.” Bioinformatics 32 (19): 3047–48.\n\n\nKrueger, Felix. 2015. “Trim Galore!: A Wrapper Around Cutadapt and FastQC to Consistently Apply Adapter and Quality Trimming to FastQ Files, with Extra Functionality for RRBS Data.” Babraham Institute.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter2.html",
    "href": "chapters/Metagenomics/Meta_chapter2.html",
    "title": "Host DNA Removal",
    "section": "",
    "text": "Download human genome index files for bowtie\nIn this step, we will remove human genomes from our raw sequences to ensure a high-quality analysis of gut microbiome. For this task, we will use bowtie2 (Langmead and Salzberg 2012) and samtools (Li 2009) tools.\nThe removal of human genome is a multi-step process. These steps are given below\nThe first step is to download bowtie2 indices for human genomes. There are multiple human genomes indices are available here. For this tutorial, we will use hg19 human genome (Thomas et al. 2019).\nThe command below download the genome zipped file and extract human genome indices.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Host DNA Removal</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter2.html#download-human-genome-index-files-for-bowtie",
    "href": "chapters/Metagenomics/Meta_chapter2.html#download-human-genome-index-files-for-bowtie",
    "title": "Host DNA Removal",
    "section": "",
    "text": "wget https://genome-idx.s3.amazonaws.com/bt/hg19.zip\nunzip hg19.zip",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Host DNA Removal</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter2.html#remove-human-genome",
    "href": "chapters/Metagenomics/Meta_chapter2.html#remove-human-genome",
    "title": "Host DNA Removal",
    "section": "Remove human genome",
    "text": "Remove human genome\nThis step consisting of three tasks:\n\nAligning raw sequences to human genomes,\nRemoving mapped sequences\nConverting resultant files back to fastq format.\n\nThe first task is executed by the following command\nbowtie2 -x hg19/hg19  -1 reads_1.fastq.gz -2 reads_2.fastq.gz -S mapped.sam --very-sensitive\nThe second task, which remove mapped sequences, is performed by the following command.\n\n# Convert sam file into bam file\nsamtools view -Sb mapped.sam &gt; mapped.bam\n\n# Extract unmapped sequences\nsamtools view -b -f 12 -F 256 mapped.bam &gt; unmapped.bam\nThe third task is executed by the following command\n# Convert bam file back to fastq format\nsamtools fastq unmapped.bam -1 unmapped_1.fastq -2 unmapped_2.fastq -0 unmapped_singletons.fastq",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Host DNA Removal</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter2.html#references",
    "href": "chapters/Metagenomics/Meta_chapter2.html#references",
    "title": "Host DNA Removal",
    "section": "References",
    "text": "References\n\n\n\n\nLangmead, Ben, and Steven L Salzberg. 2012. “Fast Gapped-Read Alignment with Bowtie 2.” Nature Methods 9 (4): 357–59.\n\n\nLi, Heng. 2009. “The Sequence Alignment/Map (SAM) Format and SAMtools 1000 Genome Project Data Processing Subgroup.” Bioinformatics 25: 1.\n\n\nThomas, Andrew Maltez, Paolo Manghi, Francesco Asnicar, Edoardo Pasolli, Federica Armanini, Moreno Zolfo, Francesco Beghini, et al. 2019. “Metagenomic Analysis of Colorectal Cancer Datasets Identifies Cross-Cohort Microbial Diagnostic Signatures and a Link with Choline Degradation.” Nature Medicine 25 (4): 667–78.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Host DNA Removal</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter3.html",
    "href": "chapters/Metagenomics/Meta_chapter3.html",
    "title": "Sequence Assembly",
    "section": "",
    "text": "References\nThis step assembles short reads into a longer one for taxonomic and functional profiling. This operation has been found to imrove performance of taxonomic assignment (Tran and Phan 2020)\nWe will employ MegaHit tool (Li et al. 2015) for assembly. This tool is memory efficient, making it an ideal choice for larger datasets. The following command performs assembly task for a single paired-read sequence.\nHere, sample_R1.fastq and sample_R2.fastq are forward and backward reads. The results are stored in megahit_output_dir specified using -o flag.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sequence Assembly</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter3.html#references",
    "href": "chapters/Metagenomics/Meta_chapter3.html#references",
    "title": "Sequence Assembly",
    "section": "",
    "text": "Li, Dinghua, Chi-Man Liu, Ruibang Luo, Kunihiko Sadakane, and Tak-Wah Lam. 2015. “MEGAHIT: An Ultra-Fast Single-Node Solution for Large and Complex Metagenomics Assembly via Succinct de Bruijn Graph.” Bioinformatics 31 (10): 1674–76.\n\n\nTran, Quang, and Vinhthuy Phan. 2020. “Assembling Reads Improves Taxonomic Classification of Species.” Genes 11 (8): 946.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Sequence Assembly</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter4.html",
    "href": "chapters/Metagenomics/Meta_chapter4.html",
    "title": "Taxonomic Profiling",
    "section": "",
    "text": "References\nIn the field of bioinformatics, taxonomy is a classification scheme which typically classifies each living organism into a heirarchical order composed of different levels such as domain, kingdom, phylum, etc. (Figure fig-taxo). This classification facilitates an understanding of micro-organism community structure at different levels, from coarse (e.g., phylum) to refine (e.g., species).\nTaxonomic profiling predics taxonomic operational units present in the metagenomic sequences at different levels of classification. To perform this task, we will employ mOTUs tool (Ruscheweyh et al. 2021).\nHere, sample_R1.fastq and sample_R2.fastq are forward and backward reads. The results are stored in motus_output_dir specified using -output flag. The database of marker genes specified by &lt;path_to_mOTUs_database&gt;.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter4.html#references",
    "href": "chapters/Metagenomics/Meta_chapter4.html#references",
    "title": "Taxonomic Profiling",
    "section": "",
    "text": "Ruscheweyh, Hans-Joachim, Alessio Milanese, Lucas Paoli, Anna Sintsova, Daniel R Mende, Georg Zeller, and Shinichi Sunagawa. 2021. “mOTUs: Profiling Taxonomic Composition, Transcriptional Activity and Strain Populations of Microbial Communities.” Current Protocols 1 (8): e218.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Taxonomic Profiling</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter5.html",
    "href": "chapters/Metagenomics/Meta_chapter5.html",
    "title": "Functional Profiling",
    "section": "",
    "text": "References\nThis step determines metabolic pathways of present microbial commnuties in gut microbiome. todo:read about metabolic pathways of gut microbial communities.\nWe will employ HUMAnN2 (Franzosa et al. 2018) for generating functional annotation of metagenomic data. This step utilises functional gene databases such as KEGG (Kyoto Encycolpedia of Genes and Genomes). The command below shows syntax for using HUMAnN2 for the task.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "chapters/Metagenomics/Meta_chapter5.html#references",
    "href": "chapters/Metagenomics/Meta_chapter5.html#references",
    "title": "Functional Profiling",
    "section": "",
    "text": "Franzosa, Eric A, Lauren J McIver, Gholamali Rahnavard, Luke R Thompson, Melanie Schirmer, George Weingart, Karen Schwarzberg Lipson, et al. 2018. “Species-Level Functional Profiling of Metagenomes and Metatranscriptomes.” Nature Methods 15 (11): 962–68.",
    "crumbs": [
      "Metagenomics Sequence Processing",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter1.html",
    "href": "chapters/Analysis/chapter1.html",
    "title": "Data loading",
    "section": "",
    "text": "Age, BMI and gender distribution\nWe will begin with a subset of dataset from Zeller et al. (2014). This dataset is from the fecal samples collected from 156 French patients. In this section, we will load the dataset and explore its basic characteristics.\nWe will also extract the bacterial species column names. Those column names starting with k__Bacteria represents bacterial species.\nFor the rest of our analysis, we will focus on five metadata features along with OTUs. Those features are age, gender, BMI, study_condition and ajcc.\nFigure fig-dist shows distributions of age, BMI, and gender of patients across different study conditions: control, adenoma, and CRC.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data loading</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter1.html#age-bmi-and-gender-distribution",
    "href": "chapters/Analysis/chapter1.html#age-bmi-and-gender-distribution",
    "title": "Data loading",
    "section": "",
    "text": "CRC patients are slightly older than control cases.\nThere is an increase in BMI for adenoma compared to control cases.\nThere are more males with CRC than females.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Age distribution\n\n\n\n\n\n\n\n\n\n\n\n(b) BMI distribution\n\n\n\n\n\n\n\n\n\n\n\n(c) Gender distribution\n\n\n\n\n\n\n\nFigure 1: Distribution across study conditions\n\n\n\n\n\n\n\nZeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Data loading</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter2.html",
    "href": "chapters/Analysis/chapter2.html",
    "title": "Species filtering",
    "section": "",
    "text": "References\nWe will filter out species whose relative abundance does not exceed 0.001 in any sample. This criterion is derived from the study by Zeller et al., the same study that provided the dataset (Zeller et al. 2014).\nAfter applying this abundance filter, the dataset is reduced to 491 species. We will now proceed with model development using these filtered species.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Species filtering</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter2.html#references",
    "href": "chapters/Analysis/chapter2.html#references",
    "title": "Species filtering",
    "section": "",
    "text": "Zeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Species filtering</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter3.html",
    "href": "chapters/Analysis/chapter3.html",
    "title": "Feature transformation",
    "section": "",
    "text": "References\nAfter filtering species on abundance criterion, we will now transform the data using a log-transformation. We will use the same transformational function that is used in Zeller et al. (2014). The transformational function is given in Equation eq-trans.\n\\[ log_{10}(x + x_0) \\tag{1}\\]\nHere\n\\(x\\) is a relative abundance value\n\\(x_0\\) is a small constant (1e-6)\nWe will apply this transformation on filtered species. The code below perform that step.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Feature transformation</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter3.html#references",
    "href": "chapters/Analysis/chapter3.html#references",
    "title": "Feature transformation",
    "section": "",
    "text": "Zeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Feature transformation</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter4.html",
    "href": "chapters/Analysis/chapter4.html",
    "title": "Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a powerful dimensionality reduction technique that reduces the number of features (dimensions) while retaining the maximum variance in the data. By capturing the most important patterns in fewer components, PCA enhances data visualization and simplifies analysis.\nWe will now apply PCA to our filtered dataset and plot the resulting components to explore potential associations with study conditions. We will apply this transformation on filtered species. The code below perform that step.\n\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# log transformed data from the previous step\ndf = microbiome_log\n\n# Step 1: Standardize the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)\n\n# Step 2: Apply PCA (reduce to 10 components for visualization)\npca = PCA(n_components=10)\npca_result = pca.fit_transform(scaled_data)\n\n# Step 3: Create a DataFrame for PCA results\npca_df = pd.DataFrame(pca_result, columns=['PCA1', 'PCA2','PCA3', 'PCA4','PCA5',\n                                           'PCA6','PCA7', 'PCA8', 'PCA9', 'PCA10'])\n\npca_df['study_condition'] = zeller_db['study_condition'].values\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) PCA1\n\n\n\n\n\n\n\n\n\n\n\n(b) PCA2\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) PCA3\n\n\n\n\n\n\n\n\n\n\n\n(d) PCA4\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) PCA5\n\n\n\n\n\n\n\n\n\n\n\n(f) PCA6\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) PCA7\n\n\n\n\n\n\n\n\n\n\n\n(h) PCA8\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i) PCA9\n\n\n\n\n\n\n\n\n\n\n\n(j) PCA910\n\n\n\n\n\n\n\nFigure 1: Principal components\n\n\n\nFigure fig-var below shows the cumulative variance captured by 10 principal components, i.e. 26%.\n\n\n\n\n\n\n\n\nFigure 2: Cumulative Explained Variance by Principal Components\n\n\n\n\n\nWe will utilize PCA components in our modeling phase to build a CRC detection system.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter5.html",
    "href": "chapters/Analysis/chapter5.html",
    "title": "Statistical analysis",
    "section": "",
    "text": "Association between host’s characteristics and CRC\nIncreasing research has found the association between microbial dysbiosis and CRC. Basing on that, we will check for differences in microbial composition of control and CRC cases. We will also investigate statistical significance of differences if found any.\nWe will now investigate for statistical differences among target classes in terms of patient demographics.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter5.html#association-between-hosts-characteristics-and-crc",
    "href": "chapters/Analysis/chapter5.html#association-between-hosts-characteristics-and-crc",
    "title": "Statistical analysis",
    "section": "",
    "text": "Age, BMI, and CRC\nWe start by investigating statistical significance of differences between age of control and CRC casese. For this test, we use Mann-Whitney test to investigate statistical significance of differences.\nThe below figures show that **there are statistical significant differences between control and CRC groups.\n\n\n\n\n\n\nAge\n\n\n\nWe found statistical differences in age of control and CRC group.\n\n\n\n\nShow the code\nfrom skbio.diversity import alpha\nfrom statannot import add_stat_annotation\n\n# functions to process the data and prepare in a format supporting phyloseq analysis\ndef get_sample_table(df):\n    df = df[metadata_colnames]\n    df = df.set_index('subjectID')\n    df.drop(['dataset_name','sampleID'],axis=1,inplace=True)\n    return df\n\n\ndef get_otu_taxa_table(df):\n    \"\"\"\n    This function returns otu table that contains relative abundance of species \n    where columns are species and rows are cases.\n    \n    Args:\n        df (dataframe): Dataframe of realtive abundance and metadata\n    \"\"\"\n    df = df[bacteria_colnames + ['subjectID']]\n    df.columns = [\"OTU_{}\".format(str(ind)) for ind, col in enumerate(bacteria_colnames)] + ['subjectID']\n    df.index = df['subjectID']\n    taxa_table = get_taxa_table(bacteria_colnames)\n    return df, taxa_table\n\n\ndef get_taxa_table(list_of_otus):\n    \"\"\"\n    This function parse all present microbial species at different heirarchy levels, e.g., class, order, phylum.\n    \n    \"\"\"\n    otu = 0\n    mapping = {}\n    taxa_cols = ['kingdom','phylum','class','order','family','genus','species']\n    df = pd.DataFrame(columns=taxa_cols)\n    otu_mapping = {}\n    otu_ids = []\n    for ind, otu in enumerate(list_of_otus):\n        tmp = {}\n        for col in taxa_cols:\n            tmp[col] = get_specific_label(otu, col)\n        tmp_df = pd.DataFrame([tmp])\n        df = pd.concat([df,tmp_df], ignore_index=[0])\n\n        otu_id = \"OTU_{}\".format(str(ind))\n        otu_mapping[otu] = otu_id\n        otu_ids.append(otu)\n\n    df['OTU'] = ['OTU_{}'.format(str(ind)) for ind in df.index]\n    df = df.set_index('OTU')\n    return df\n\n\ndef get_specific_label(l, t):\n    \"\"\"\n    This function parse the taxonomic assignment lable and fetch the specified information (e.g., kingdom, family)\n\n    Args:\n        l (str): string of taxonomy\n        t (str): string specifying the requested information (e.g., kingdom, family, genus, etc.)\n\n    Returns:\n        str: requested heirarichal info \n    \"\"\"\n    taxa_order = {'kingdom':0,'phylum':1,'class':2,'order':3,'family':4,'genus':5,'species':6}\n\n    try:\n        specific_label = l.split('|')[taxa_order[t]]\n\n        return specific_label.strip().split('__')[1]\n    except:\n        return 'Unknown'\n\n    \ndef get_otu_detail(taxa_table, otu_label, rank):\n    return taxa_table[otu_label][rank]\n\n\ndef aggregate_by_taxonomy(otu_table, taxa_table, taxa_rank):\n    \"\"\"\n    This function aggregates data based on specified \n    taxa rank (e.g., kingdom, class, order, phylum, genus, species).\n    \"\"\"\n    unique_values = (taxa_table[taxa_rank].unique())\n\n    # mapping for otus to unique value of chosen taxa rank\n    taxa_to_otu = {}\n\n    # prepare the mapping\n    for unique_value in unique_values:\n        tdf = taxa_table.loc[taxa_table[taxa_rank] == unique_value, :]\n        otus = tdf.index.to_list()   \n        taxa_to_otu[unique_value.strip()] = otus\n    \n    # create a dictionary for formulating expressions\n    taxa_to_exp = {}\n\n    for key in taxa_to_otu.keys():\n        taxa_to_exp[key] = '{} = 0.000001'.format(key)\n        for otu in taxa_to_otu[key]:\n            taxa_to_exp[key] += ' + ' + otu\n            otu_table[otu] = pd.to_numeric(otu_table[otu], errors='coerce')\n\n    agg_df = otu_table\n    \n    for key, expr in taxa_to_exp.items():\n        agg_df[key] = 0\n        agg_df = agg_df.eval(expr, engine='python')\n        \n    agg_df = agg_df[list(unique_values)]\n    \n    return agg_df   \n\n\ndef extend_with_alpha(df, metadata_features):\n    \"\"\"\n    This function extends the dataframe with alpha diversity measures.\n    \n    Args:\n        df: dataframe\n        \n        metadata_features: list of metadata feature names\n        \n    Returns:\n        dataframe: extended dataframe with alpha diversity features\n    \"\"\"\n    diversity_measures = pd.DataFrame()\n\n    alpha_diversity_metrics = [\n        \"chao1\",\n        \"shannon\",\n        \"simpson\",\n        \"simpson_e\",\n        \"fisher_alpha\",\n        \"berger_parker\"\n    ]\n\n    shannon_diversity = df.apply(lambda x: alpha.shannon(x), axis=1)\n    chao1_diversity   = df.apply(lambda x: alpha.chao1(x), axis=1)\n    simpson_diversity   = df.apply(lambda x: alpha.simpson(x), axis=1)\n    simpson_e_diversity   = df.apply(lambda x: alpha.simpson_e(x), axis=1)\n    fisher_diversity   = df.apply(lambda x: alpha.fisher_alpha(x), axis=1)\n    berger_parker_diversity   = df.apply(lambda x: alpha.berger_parker_d(x), axis=1)\n\n    diversity_measures['shannon'] = shannon_diversity\n    diversity_measures['chao1'] = chao1_diversity\n    diversity_measures['simpson'] = simpson_diversity\n    diversity_measures['simpson_e'] = simpson_e_diversity\n    diversity_measures['fisher_alpha'] = fisher_diversity\n    diversity_measures['berger_parker'] = berger_parker_diversity\n    \n    X_alpha = diversity_measures.reset_index().drop(['subjectID'], axis=1)\n    X_extended = pd.concat([metadata_features,X_alpha],axis=1)\n    \n    return X_extended\n\n# convert data tables into otu and taxa table\notu_table, taxa_table = get_otu_taxa_table(zeller_db)\n\n# aggregating data at higher levels\nphylum_agg = aggregate_by_taxonomy(otu_table, taxa_table, 'phylum')\ngenus_agg = aggregate_by_taxonomy(otu_table, taxa_table, 'genus')\norder_agg = aggregate_by_taxonomy(otu_table, taxa_table, 'order')\n\ncolor_palette = {'control':'green',\n                'adenoma':'orange',\n                'CRC':'#c80000'}\n\norder = ['control', 'adenoma', 'CRC']\nx = 'study_condition'\n\npairs = [\n    ('control','adenoma'),\n    ('control','CRC'),\n    ('adenoma','CRC'),\n]\n\nmetadata = zeller_db[metadata_colnames]\n\n# changing data type of age and BMI\nmetadata['age'] = pd.to_numeric(metadata.age, errors='coerce')\nmetadata['BMI'] = pd.to_numeric(metadata.BMI, errors='coerce')\n\n\n\nShow the code\n# plotting distribution\n\nfor ind, y in enumerate(['age','BMI']):\n    plt.figure()\n    ax = sns.boxplot(data=metadata, y=y,x=x, palette=color_palette, order= order)\n    #annot = Annotator(ax, pairs=pairs, data=metadata, x=x, y=y, hue=x, hue_order=order, order=order)\n    ax, test_results = add_stat_annotation(ax, box_pairs=pairs, data=metadata, x=x, y=y,\n                                           hue_order=order, order=order,\n                                           test='Mann-Whitney', text_format='star',comparisons_correction=None, \n                                           loc='inside', verbose=False)\n\n    plt.title(f'{y.upper()}')\n\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n(a) Age\n\n\n\n\n\n\n\n\n\n\n\n(b) BMI\n\n\n\n\n\n\n\nFigure 1: Age/BMI distribution\n\n\n\n\n\nGender and CRC\nWe now check the distribution of gender across different target groups (i.e., control, adenoma, CRC). We employ the Chi-squared test to investigate the statistical significance of differences in gender distribution across different groups.\nFigure fig-stat-gender shows the frequency count of males/females across control, adenoma, and CRC groups. The differences were found to be statistically significant (p-value &lt; .05).\n\n\n\n\n\n\nGender\n\n\n\nWe also found statistical differences in gender between control, adenoma and CRC group.\n\n\n\n\nShow the code\nfrom scipy.stats import chi2_contingency\n\n# Create a contingency table\ncontingency_table = pd.crosstab(metadata['gender'], metadata['study_condition'])\n\n# Apply Fisher's Exact Test\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\n\n# new figure\nplt.figure()\n\n# plot frequency plot\nsns.countplot(data=metadata, x='study_condition',hue='gender')\n\n# add p-value\nplt.text(0.5, 32, f'$X^2 test$ p-value: {p_value:.4f}', fontsize=12, color='blue')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Gender distribution",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter5.html#association-between-hosts-characteristics-and-species-abundance",
    "href": "chapters/Analysis/chapter5.html#association-between-hosts-characteristics-and-species-abundance",
    "title": "Statistical analysis",
    "section": "Association between host’s characteristics and species abundance",
    "text": "Association between host’s characteristics and species abundance\nWe will now explore for association between host’s characteristics (e.g., age, BMI) and species relative abundace. Figure fig-stat-corr below shows correlation between host’s characteristics and phylum level abundance data.\n\nFirnicutes, Spirochaetes, and Verrucomicrobia phylum levels are negatively correlated with age. That means as a person gets older these three phylum levels tend to get decreased.\nIn case of BMI, Spirochaetes, Firnicutes, Deferribacteres, Bacteroidetes, and Actinobacteria are found negatively correlated. That implies an increase in BMI (which could be taken as an indication of obesity) is associated with decrease in those phylums.\n\n\nShow the code\n# plotting distribution\n\ndef extend_abundance_metadata(df,meta):\n    \"\"\"\n    This function extends abundance data with metadata information.\n    \n    Args:\n        df (DataFrame): relative abundance data\n        meta (DataFrame): host's characteristics\n        \n    Returns:\n        DataFrame\n    \"\"\"\n    return pd.concat([df,meta],axis=1)\n\n# relative abundance aggregation at \n#family_abundance = aggregate_by_taxonomy(otu_table, taxa_table, 'family')\ngenus_abundance = aggregate_by_taxonomy(otu_table, taxa_table, 'genus')\nphylum_abundance = aggregate_by_taxonomy(otu_table, taxa_table, 'phylum')\nmetadata_ = metadata.set_index(metadata['subjectID'])\n\n# plot age correlation plot\nphylum_metadata = extend_abundance_metadata(phylum_abundance,metadata_[['age']])\nphylum_corr = phylum_metadata.corr()\nplt.figure(figsize=(7,10))\ndata_plot = phylum_corr['age'].drop('age')\nbars = plt.barh(data_plot.index, data_plot, color=np.where(data_plot &gt; 0, 'green', 'red'))\nplt.xlim([-.5,.5])\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.title('Age')\nplt.show()\n\n# plot bmi correlation plot\nphylum_metadata = extend_abundance_metadata(phylum_abundance,metadata_[['BMI']])\nphylum_corr = phylum_metadata.corr()\nplt.figure(figsize=(7,10))\ndata_plot = phylum_corr['BMI'].drop('BMI')\nbars = plt.barh(data_plot.index, data_plot, color=np.where(data_plot &gt; 0, 'green', 'red'))\nplt.xlim([-.5,.5])\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.title('BMI')\nplt.show()\n    \nphylum_metadata_ = phylum_metadata.copy()\nphylum_metadata_['study_condition'] = metadata_['study_condition'].apply(\n    lambda x: 'malign' if x == 'CRC' else 'benign')\n\ndf = phylum_metadata_.melt(id_vars='study_condition',value_vars=phylum_agg.columns)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Correlation between age and phylum level abundance\n\n\n\n\n\n\n\n\n\n\n\n(b) Correlation between BMI and phylum level abundance\n\n\n\n\n\n\n\nFigure 3: Correlation with phylum abundace\n\n\n\nFigure fig-crc-phylum (c) below shows differences in microbial composition in terms of phylum level abundance among benign and malign tumors. We can notice three phylums differ among benign and malign tumor groups. Those phylums are Proteobacteria, Firnicutes, and Bacteroidetes.\n\nWe combined control with adenoma to create benign tumor class, and CRC class renamed as malign tumor.\n\n\n\nShow the code\nplt.figure(figsize=(7,10))\nsns.boxplot(data=df, y='variable',x='value', order=list(data_plot.index)[::-1], hue='study_condition', palette={'benign':'green','malign':'red'})\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: CRC and phylum abundace",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter5.html#exploring-barteroidetes-firmicutes-and-proteobacteria-for-theis-association-with-crc",
    "href": "chapters/Analysis/chapter5.html#exploring-barteroidetes-firmicutes-and-proteobacteria-for-theis-association-with-crc",
    "title": "Statistical analysis",
    "section": "Exploring Barteroidetes, Firmicutes, and Proteobacteria for theis association with CRc",
    "text": "Exploring Barteroidetes, Firmicutes, and Proteobacteria for theis association with CRc\nWe go further checking whether these differences are statistically significant or not. We employ Mann-Whitney test which is a non-parametric test for checking significance of differences in values from two independent groups.\nFigure fig-differ_phylum below shows distributions of abundance at phylum levels across benign and malign cases for all three selected phylumns. The differences were found to be statistically significant.\n\n\nShow the code\ncolor_palette = {'benign':'green','malign':'red'}\n\n# selected phylums for statistical analysis\nselected_phylums = ['Firmicutes','Proteobacteria','Bacteroidetes']\n\n# extracting only selected phylum data\ndf_selected = df.loc[df['variable'].isin(selected_phylums),:]\n\n# pairs for statistical test\npairs = [\n    (('Firmicutes','benign'), ('Firmicutes','malign')),\n    (('Proteobacteria','benign'), ('Proteobacteria','malign')),\n    (('Bacteroidetes','benign'), ('Bacteroidetes','malign'))\n]\n\n# creating a new figure\nplt.figure()\n\n# plotting boxplot\nax = sns.boxplot(data=df_selected, x='variable', y='value',hue='study_condition', palette=color_palette)\n\n# adding statistical annotation from Mann-Whitney test\nax, test_results = add_stat_annotation(ax, box_pairs=pairs, data=df_selected, x='variable', y='value', \n                                       hue='study_condition',hue_order=['benign','malign'], \n                                       \n                                       test='Mann-Whitney', text_format='star',comparisons_correction=None, \n                                       loc='inside', verbose=False)\n\nplt.xlabel('Phylums')\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Phylum differences among benign and malign cases\n\n\n\n\n\nLets go further and check which genus and species under these phylums are statistically different in terms of relative abundance among benign and malign tumors.\n\n\nShow the code\ntaxa_selected = taxa_table.loc[taxa_table['phylum'].isin(selected_phylums),:]\n\n# fetching species related to selected phylums\nselected_species_cols = list(taxa_table['species'].unique())\n\n# fetching genus related to selected phylums\nselected_genus_cols = list(taxa_table['genus'].unique())\n\n# fetching family related to selected phylums\nselected_family_cols = list(taxa_table['family'].unique())\n\n# fetching order related to selected phylums\nselected_order_cols = list(taxa_table['order'].unique())\n\n# fetching order related to selected phylums\nselected_class_cols = list(taxa_table['class'].unique())\n\n\n\n\nShow the code\nfrom statannotations.Annotator import Annotator\n\ndef plot_selected_taxa(selected_taxa, plot_at,figsize=(7,15),log_scale=False,title=\"\"):\n    \"\"\"\n    Args:\n    ----\n        selected_taxa(str): taxa which are selected for further exploration\n        taxa_abun_df (dataframe): relative abundance data at taxa\n        plot_at (str): taxa at which distribution will be plotted for benign and malign tumors\n    \n    \n    \"\"\"\n    df_abundance = aggregate_by_taxonomy(otu_table, taxa_table, plot_at)\n    metadata_ = metadata.set_index(metadata['subjectID'])\n\n    taxa_abundance_selected = df_abundance[selected_taxa]\n\n    taxa_abundance_selected['study_condition'] = metadata_['study_condition'].apply(\n    lambda x: 'malign' if x == 'CRC' else 'benign')\n\n    \n    pairs = []\n    \n    for col in taxa_abundance_selected.columns:\n        if col != 'study_condition':\n            pairs.append(((col,'benign'),(col,'malign')))\n\n    plt.figure(figsize=figsize)\n    plot_df = taxa_abundance_selected.melt(id_vars='study_condition',value_vars=selected_taxa)\n    \n    ax = sns.boxplot(data=plot_df,x='variable',y='value',hue='study_condition', palette=color_palette)\n    add_stat_annotation(ax, box_pairs=pairs, data=plot_df, x='variable', y='value', \n                                       hue='study_condition',hue_order=['benign','malign'], \n                                       \n                                       test='Mann-Whitney', text_format='star',comparisons_correction=None, \n                                       loc='inside', verbose=False)  \n    ax.set_ylabel(plot_at)\n    plt.xticks(rotation='vertical')\n    if log_scale:\n        ax.set_yscale('log')\n    plt.title(title)\n    plt.show()\n\n\n\n\nShow the code\nplot_selected_taxa(selected_family, plot_at='family', figsize=(50,5),title='families from selected phylums')\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_selected_taxa(selected_genus, plot_at='genus', figsize=(80,5),title='genus from selected phylums')",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter6.html",
    "href": "chapters/Analysis/chapter6.html",
    "title": "Building a CRC classifier",
    "section": "",
    "text": "Target class distribution across cohorts\nNow, we will build our CRC classifier using gut microbial data from four different cohorts: French, Italian, Austrian, and Germany. Our dataset consists of relative abundance of gut microbial communities. The dataset also cotains classification of those communities at different taxonomic levels (e.g., phylum, genus, species).\nWe will follow the approach depicted in Figure fig-method for building our CRC classifier. We will divide all cohorts using 70/30 split into training and test set. All training sets will be utilized for building models and selecting the one with best performance. The selected model then applied on test set from different cohorts to assess generalizability of the model across cohorts.\nFigure fig-dist-class below shows the ditribution of target class. For our modeling task, we have grouped adenoma and control into a single class of benign tumor. While, CRC class is treated as malign tumor.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter6.html#target-class-distribution-across-cohorts",
    "href": "chapters/Analysis/chapter6.html#target-class-distribution-across-cohorts",
    "title": "Building a CRC classifier",
    "section": "",
    "text": "French and Austrian cohorts are highly skewed in terms of target class distribution, both having ~ 30% cases of CRC.\nWhile, German and Italian cohorts have relatively balanced cases of malign and benign tumors.\n\n\nShow the code\nfor country in ['france','austria','germany','italy']:\n    plt.figure()\n    \n    # Get country specific dataset\n    dataset = get_country_dataset('Nine_CRC_cohorts_taxon_profiles.tsv',country,country_dataset_mapping)\n\n    # Plot class ditribution\n    sns.countplot(data=dataset, x='target_class',alpha=.6,order=['benign','malign'],palette={'benign':'green','malign':'red'})\n    plt.ylim([0,130])\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) French cohort\n\n\n\n\n\n\n\n\n\n\n\n(b) Austrian cohort\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) German cohort\n\n\n\n\n\n\n\n\n\n\n\n(d) Italian cohort\n\n\n\n\n\n\n\nFigure 2: Target class distribution in cohorts",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter6.html#train-and-test-split-of-dataset",
    "href": "chapters/Analysis/chapter6.html#train-and-test-split-of-dataset",
    "title": "Building a CRC classifier",
    "section": "Train and test split of dataset",
    "text": "Train and test split of dataset\nWe will split each of aforementioned cohort using 70/30 split rule resulting in a training set of 70% cases and a test set of 30% cases. We will use the test set only for our final evaluation of our models’ performance.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\n\n# List to store training and test seperately\ntrain_X_data = []\ntest_X_data = []\n\nfor country in ['france','austria','germany','italy']:\n    # Get country specific dataset\n    dataset = get_country_dataset('Nine_CRC_cohorts_taxon_profiles.tsv',country,country_dataset_mapping)\n\n    y = dataset['target_class']\n    X = dataset.copy()\n    \n    # Split train and test\n    train_x, test_x, train_y, test_y = train_test_split(X,y,test_size=0.20, random_state=42)\n    \n    # Store country\n    train_x['groups'] = country\n    test_x['groups'] = country\n    \n    # Storing train and test seperately\n    train_X_data.append(train_x)\n    test_X_data.append(test_x)\n    \n# Concatenating all country's dataset\ntrain_X_df = pd.concat(train_X_data, axis=0)\ntest_X_df = pd.concat(test_X_data, axis=0)\n\n# Preparing X and y\ntrain_X = train_X_df.drop(['target_class'], axis=1)\ntrain_y = train_X_df['target_class']\n\ntest_X_df.to_csv('test_X_df.csv',index=False)",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter6.html#model-building-pipeline",
    "href": "chapters/Analysis/chapter6.html#model-building-pipeline",
    "title": "Building a CRC classifier",
    "section": "Model building pipeline",
    "text": "Model building pipeline\nThe below diagram depicts our model building pipeline consisting of steps ranging from feature selection, dimensionality reduction to model selection. These steps are explained at length below.\n\n\n\n\n\ngraph TD\n    A(Input Data) --&gt; B1(Relative Abundance Filter)\n    B1 --&gt; I(Variance Threshold Filter)\n    I --&gt; C1(Feature Selection)\n    A --&gt; B2(Compute Alpha Diversity)\n    A --&gt; B3(Impute Missing Metadata Features)\n    A --&gt; B4(PCA)\n    C1 --&gt; D(Combine Features)\n    B2 --&gt; D\n    B3 --&gt; D\n    B4 --&gt; D\n    D --&gt; E(Train ML Model with Nested CV)\n    E --&gt; H(Model Selection)\n\n\n\n\n\n\n\n\n\nShow the code\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import RFECV, SelectKBest, SelectFpr, f_classif, VarianceThreshold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.utils import resample\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, roc_curve, auc\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n\n# Creating custom pipeline processor\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based of feature names for pipeline\n    \"\"\"\n    def __init__(self, variables):\n        self.variables = variables\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        X_dropped = X[self.variables]\n        return X_dropped\n    \n    \nclass RelativeAbundanceFilter(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based on relative abundance filtering\n    \"\"\"\n    def __init__(self, threshold):\n        self.threshold = threshold\n        self.columns_select = None\n        \n    def fit(self, X, y=None):\n        self.columns_to_select = X.columns[X.max(axis=0) &gt; self.threshold]\n        return self\n    \n    def transform(self, X):\n        X_selected = X[self.columns_to_select]\n        return X_selected\n    \n    \nclass VarianceThresholdFilter(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based on relative abundance filtering\n    \"\"\"\n    def __init__(self, threshold=0.0):\n        self.threshold = threshold\n        self.transformer = VarianceThreshold(self.threshold)\n        \n    def fit(self, X, y=None):\n        self.transformer.fit(X,y)\n        return self\n    \n    def transform(self, X):\n        X_filtered = self.transformer.transform(X)\n        return X_filtered\n    \n    \nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Select top k features based on specified strategy\n    \"\"\"\n    def __init__(self, cv=10):\n        self.estimator = LassoCV(cv=cv)\n        self.feature_coef = pd.DataFrame()\n        \n    def fit(self, X, y=None):\n        self.feature_coef.columns = X.columns\n        \n        for sample in resample(X,y):\n            self.estimator.fit(X,y, random_state=42)\n            coef_df = pd.DataFrame(self.estimator.coef_, columns=X.columns)\n            self.feature_coef = pd.concat([self.feature_coef, coef_df], axis=0, ignore_index=True)\n            \n        return self\n    \n    def transform(self, X):\n        X_selected = self.feature_selector.transform(X)\n        df = pd.DataFrame(X_selected, columns=self.feature_names_out)\n        \n        return df\n    \n        \nclass AlphaFeatureExtender(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer to extend relative abundace data with alpha diversity measure\n    \"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Create an empty dataframe\n        diversity_measures = pd.DataFrame()\n\n        # alpha diversty measures\n        alpha_diversity_metrics = [\n            \"chao1\",\n            \"shannon\",\n            \"simpson\",\n            \"simpson_e\",\n            \"fisher_alpha\",\n            \"berger_parker\"\n        ]\n\n        # Compute alpha diversity measures\n        shannon_diversity = X.apply(lambda x: alpha.shannon(x), axis=1)\n        chao1_diversity   = X.apply(lambda x: alpha.chao1(x), axis=1)\n        simpson_diversity   = X.apply(lambda x: alpha.simpson(x), axis=1)\n        simpson_e_diversity   = X.apply(lambda x: alpha.simpson_e(x), axis=1)\n        fisher_diversity   = X.apply(lambda x: alpha.fisher_alpha(x), axis=1)\n        berger_parker_diversity   = X.apply(lambda x: alpha.berger_parker_d(x), axis=1)\n\n        # Add alpha measures to the dataframe\n        diversity_measures['shannon'] = shannon_diversity\n        diversity_measures['chao1'] = chao1_diversity\n        diversity_measures['simpson'] = simpson_diversity\n        diversity_measures['simpson_e'] = simpson_e_diversity\n        diversity_measures['fisher_alpha'] = fisher_diversity\n        diversity_measures['berger_parker'] = berger_parker_diversity\n\n        return diversity_measures\n\n    \nclass MetaDataImputer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Imputer for specific metadata (i.e., age, genger, BMI)\n    \"\"\"\n    def __init__(self):\n        self.fill_values = dict()\n\n    \n    def fit(self, X, y=None):\n        X['age'] = pd.to_numeric(X['age'], errors='coerce')\n        X['BMI'] = pd.to_numeric(X['BMI'], errors='coerce')\n        \n        self.fill_values['age'] = X['age'].mean()\n        self.fill_values['BMI'] = X['BMI'].mean()\n        self.fill_values['gender'] = X['gender'].mode().values[0]\n        return self\n    \n    def transform(self, X):\n        if X['age'].dtype == 'O':\n            X['age'] = pd.to_numeric(X['age'], errors='coerce')\n\n        if X['BMI'].dtype == 'O':\n            X['BMI'] = pd.to_numeric(X['BMI'], errors='coerce')\n        \n        for col in X.columns:\n            X[col] = X[col].fillna(self.fill_values[col])\n        X['gender'] = X['gender'].map({'female':0, 'male':1})\n        return X\n    \n     \ndef prepare_dataset(X, return_mapping=False):\n    \"\"\"\n    This function prepare dataframe for modeling task\n    \"\"\"\n    \n    otu, taxa = get_otu_table(X)   \n    metadata = get_metadata(X,['age','gender','BMI'])\n\n    for col in otu.columns:\n        otu[col] = otu[col].astype('float32')\n        \n    metadata['age'] = pd.to_numeric(metadata['age'], errors='coerce')\n    metadata['BMI'] = pd.to_numeric(metadata['BMI'], errors='coerce')\n    \n    y= X['target_class'].map({'benign':0,'malign':1})\n    y.reset_index(drop=True, inplace=True)    \n    otu.reset_index(drop=True, inplace=True)\n    \n    otu_to_species = taxa['species'].to_dict()\n    \n    X_ = pd.concat([otu, metadata], axis=1)\n    if return_mapping:\n        return X_, y, otu_to_species\n    else:\n        return X_, y\n\n\n\nApplying filters to reduce feature space\nWe will start by reducing feature spaces through the following three steps.\n\nRelative abundance filter\nVariance threshold filter\nBest features selection using Lasso\n\n\n\nShow the code\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.decomposition import PCA\n\ndef log(x):\n    return np.log(x+.000001)\n\n# Prepare dataset for modeling\nX, y, m = prepare_dataset(train_X_df, return_mapping=True)\n\n# Create list of otus columns\notu_columns = [item for item in X.columns if item not in ['age','gender','BMI']]\n\n# Pipeline for filtering\nabundance_processing =  Pipeline([\n                    ('abundance',ColumnSelector(otu_columns)),\n                    ('relative',RelativeAbundanceFilter(threshold=.001)),\n                    ('variance', VarianceThresholdFilter()),\n                    ('log', FunctionTransformer(log)),\n                    ('standard', MinMaxScaler())\n                    ])\n\n# Pipeline for computing alpha diversity\nalpha_processing = Pipeline([\n    ('abundance', ColumnSelector(otu_columns)),\n    ('alpha_extended', AlphaFeatureExtender())\n])\n\n# Pipeline for imputing metadata\nmeta_processing = Pipeline([\n    ('meta_columns', ColumnSelector(['age','BMI','gender'])),\n    ('meta_imputer', MetaDataImputer())\n])\n\n# Pipeline for PCA\npca_processing = Pipeline([\n    ('abundance',ColumnSelector(otu_columns)),\n    ('pca',PCA()),\n])\n\n# Concatenating results \ncombined_features = FeatureUnion([\n    ('abun_part', abundance_processing),\n    ('alpha_part', alpha_processing),\n    ('meta_part', meta_processing),\n    ('pca_part',pca_processing),\n])\n\n# Create full pipeline\nfull_pipeline = Pipeline([\n    ('combined_part', combined_features),\n    ('select_part', ColumnSelector(final_features)),\n    ('train', LogisticRegression(C=.1, penalty='l1',solver='liblinear')),\n     ]\n)\n\nfull_pipeline.fit(X,y)\n\n\nPipeline(steps=[('combined_part',\n                 FeatureUnion(transformer_list=[('abun_part',\n                                                 Pipeline(steps=[('abundance',\n                                                                  ColumnSelector(variables=['OTU_0',\n                                                                                            'OTU_1',\n                                                                                            'OTU_2',\n                                                                                            'OTU_3',\n                                                                                            'OTU_4',\n                                                                                            'OTU_5',\n                                                                                            'OTU_6',\n                                                                                            'OTU_7',\n                                                                                            'OTU_8',\n                                                                                            'OTU_9',\n                                                                                            'OTU_10',\n                                                                                            'OTU_11',\n                                                                                            'OTU_12',\n                                                                                            'OTU_13',\n                                                                                            'OTU_14',\n                                                                                            'OTU_15',\n                                                                                            'OTU_16',\n                                                                                            'OTU_17',\n                                                                                            'OTU_18',\n                                                                                            'OTU_19',\n                                                                                            'OTU_20',\n                                                                                            'OTU_21',\n                                                                                            'OTU_22',\n                                                                                            'OTU_23',\n                                                                                            'OTU_24',\n                                                                                            'OT...\n                 ColumnSelector(variables=['OTU_16', 'OTU_28', 'OTU_108',\n                                           'OTU_111', 'OTU_144', 'OTU_153',\n                                           'OTU_157', 'OTU_158', 'OTU_161',\n                                           'OTU_178', 'OTU_218', 'OTU_348',\n                                           'OTU_353', 'OTU_367', 'OTU_372',\n                                           'OTU_375', 'OTU_378', 'OTU_381',\n                                           'OTU_388', 'OTU_446', 'OTU_464',\n                                           'OTU_465', 'OTU_466', 'OTU_468',\n                                           'OTU_470', 'OTU_503', 'OTU_535',\n                                           'OTU_544', 'OTU_546', 'OTU_547', ...])),\n                ('train',\n                 LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('combined_part',\n                 FeatureUnion(transformer_list=[('abun_part',\n                                                 Pipeline(steps=[('abundance',\n                                                                  ColumnSelector(variables=['OTU_0',\n                                                                                            'OTU_1',\n                                                                                            'OTU_2',\n                                                                                            'OTU_3',\n                                                                                            'OTU_4',\n                                                                                            'OTU_5',\n                                                                                            'OTU_6',\n                                                                                            'OTU_7',\n                                                                                            'OTU_8',\n                                                                                            'OTU_9',\n                                                                                            'OTU_10',\n                                                                                            'OTU_11',\n                                                                                            'OTU_12',\n                                                                                            'OTU_13',\n                                                                                            'OTU_14',\n                                                                                            'OTU_15',\n                                                                                            'OTU_16',\n                                                                                            'OTU_17',\n                                                                                            'OTU_18',\n                                                                                            'OTU_19',\n                                                                                            'OTU_20',\n                                                                                            'OTU_21',\n                                                                                            'OTU_22',\n                                                                                            'OTU_23',\n                                                                                            'OTU_24',\n                                                                                            'OT...\n                 ColumnSelector(variables=['OTU_16', 'OTU_28', 'OTU_108',\n                                           'OTU_111', 'OTU_144', 'OTU_153',\n                                           'OTU_157', 'OTU_158', 'OTU_161',\n                                           'OTU_178', 'OTU_218', 'OTU_348',\n                                           'OTU_353', 'OTU_367', 'OTU_372',\n                                           'OTU_375', 'OTU_378', 'OTU_381',\n                                           'OTU_388', 'OTU_446', 'OTU_464',\n                                           'OTU_465', 'OTU_466', 'OTU_468',\n                                           'OTU_470', 'OTU_503', 'OTU_535',\n                                           'OTU_544', 'OTU_546', 'OTU_547', ...])),\n                ('train',\n                 LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))])  combined_part: FeatureUnion?Documentation for combined_part: FeatureUnionFeatureUnion(transformer_list=[('abun_part',\n                                Pipeline(steps=[('abundance',\n                                                 ColumnSelector(variables=['OTU_0',\n                                                                           'OTU_1',\n                                                                           'OTU_2',\n                                                                           'OTU_3',\n                                                                           'OTU_4',\n                                                                           'OTU_5',\n                                                                           'OTU_6',\n                                                                           'OTU_7',\n                                                                           'OTU_8',\n                                                                           'OTU_9',\n                                                                           'OTU_10',\n                                                                           'OTU_11',\n                                                                           'OTU_12',\n                                                                           'OTU_13',\n                                                                           'OTU_14',\n                                                                           'OTU_15',\n                                                                           'OTU_16',\n                                                                           'OTU_17',\n                                                                           'OTU_18',\n                                                                           'OTU_19',\n                                                                           'OTU_20',\n                                                                           'OTU_21',\n                                                                           'OTU_22',\n                                                                           'OTU_23',\n                                                                           'OTU_24',\n                                                                           'OTU_25',\n                                                                           'OTU_26',\n                                                                           'OTU_27',\n                                                                           'OTU_28',...\n                               ('pca_part',\n                                Pipeline(steps=[('abundance',\n                                                 ColumnSelector(variables=['OTU_0',\n                                                                           'OTU_1',\n                                                                           'OTU_2',\n                                                                           'OTU_3',\n                                                                           'OTU_4',\n                                                                           'OTU_5',\n                                                                           'OTU_6',\n                                                                           'OTU_7',\n                                                                           'OTU_8',\n                                                                           'OTU_9',\n                                                                           'OTU_10',\n                                                                           'OTU_11',\n                                                                           'OTU_12',\n                                                                           'OTU_13',\n                                                                           'OTU_14',\n                                                                           'OTU_15',\n                                                                           'OTU_16',\n                                                                           'OTU_17',\n                                                                           'OTU_18',\n                                                                           'OTU_19',\n                                                                           'OTU_20',\n                                                                           'OTU_21',\n                                                                           'OTU_22',\n                                                                           'OTU_23',\n                                                                           'OTU_24',\n                                                                           'OTU_25',\n                                                                           'OTU_26',\n                                                                           'OTU_27',\n                                                                           'OTU_28',\n                                                                           'OTU_29', ...])),\n                                                ('pca', PCA())]))]) abun_partColumnSelectorColumnSelector(variables=['OTU_0', 'OTU_1', 'OTU_2', 'OTU_3', 'OTU_4', 'OTU_5',\n                          'OTU_6', 'OTU_7', 'OTU_8', 'OTU_9', 'OTU_10',\n                          'OTU_11', 'OTU_12', 'OTU_13', 'OTU_14', 'OTU_15',\n                          'OTU_16', 'OTU_17', 'OTU_18', 'OTU_19', 'OTU_20',\n                          'OTU_21', 'OTU_22', 'OTU_23', 'OTU_24', 'OTU_25',\n                          'OTU_26', 'OTU_27', 'OTU_28', 'OTU_29', ...]) RelativeAbundanceFilterRelativeAbundanceFilter(threshold=0.001) VarianceThresholdFilterVarianceThresholdFilter()  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(func=&lt;function log at 0x3126e0f40&gt;)  MinMaxScaler?Documentation for MinMaxScalerMinMaxScaler() alpha_partColumnSelectorColumnSelector(variables=['OTU_0', 'OTU_1', 'OTU_2', 'OTU_3', 'OTU_4', 'OTU_5',\n                          'OTU_6', 'OTU_7', 'OTU_8', 'OTU_9', 'OTU_10',\n                          'OTU_11', 'OTU_12', 'OTU_13', 'OTU_14', 'OTU_15',\n                          'OTU_16', 'OTU_17', 'OTU_18', 'OTU_19', 'OTU_20',\n                          'OTU_21', 'OTU_22', 'OTU_23', 'OTU_24', 'OTU_25',\n                          'OTU_26', 'OTU_27', 'OTU_28', 'OTU_29', ...]) AlphaFeatureExtenderAlphaFeatureExtender() meta_partColumnSelectorColumnSelector(variables=['age', 'BMI', 'gender']) MetaDataImputerMetaDataImputer() pca_partColumnSelectorColumnSelector(variables=['OTU_0', 'OTU_1', 'OTU_2', 'OTU_3', 'OTU_4', 'OTU_5',\n                          'OTU_6', 'OTU_7', 'OTU_8', 'OTU_9', 'OTU_10',\n                          'OTU_11', 'OTU_12', 'OTU_13', 'OTU_14', 'OTU_15',\n                          'OTU_16', 'OTU_17', 'OTU_18', 'OTU_19', 'OTU_20',\n                          'OTU_21', 'OTU_22', 'OTU_23', 'OTU_24', 'OTU_25',\n                          'OTU_26', 'OTU_27', 'OTU_28', 'OTU_29', ...])  PCA?Documentation for PCAPCA() ColumnSelectorColumnSelector(variables=['OTU_16', 'OTU_28', 'OTU_108', 'OTU_111', 'OTU_144',\n                          'OTU_153', 'OTU_157', 'OTU_158', 'OTU_161', 'OTU_178',\n                          'OTU_218', 'OTU_348', 'OTU_353', 'OTU_367', 'OTU_372',\n                          'OTU_375', 'OTU_378', 'OTU_381', 'OTU_388', 'OTU_446',\n                          'OTU_464', 'OTU_465', 'OTU_466', 'OTU_468', 'OTU_470',\n                          'OTU_503', 'OTU_535', 'OTU_544', 'OTU_546', 'OTU_547', ...])  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(C=0.1, penalty='l1', solver='liblinear') \n\n\n\n\nShow the code\nfrom imblearn.over_sampling import RandomOverSampler\nfrom numpy.random import RandomState\nfrom sklearn.feature_selection import SelectKBest\n\n# Feature selection process\ndf_coef = pd.DataFrame()\n\nfeature_set = []\n\nfor i in range(100):\n    sampler = RandomOverSampler(sampling_strategy=np.random.choice([1]), random_state=RandomState())\n    sample_X, sample_y = sampler.fit_resample(X_features, y)\n\n    sel = SelectKBest(k=100)\n    X_selected = sel.fit_transform(sample_X, sample_y)\n\n    feature_set += list(X_selected.columns)\n \n#import collections\n#counter = collections.Counter(feature_set)\n#final_features = [item[0] for item in counter.most_common(40)]\n\nX_final = X_features[final_features]\n\nlasso_param_grid = {\n    \"lasso__C\": [0.001, .01, .1, 1, 10, 100],\n}\n\ndt_param_grid = {\n    \"dt__max_depth\": [3,4,5,6],\n    \"dt__criterion\":['gini','entropy'],\n    \"dt__min_samples_split\":[2,4,6,8,10]\n}\n\nrf_param_grid = {\n    \"rf__n_estimators\":[50,100,150,200],\n    \"rf__max_depth\":[3,4,5,6],\n    \n}\n\nlasso_pipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('lasso', LogisticRegression(penalty='l1',solver='liblinear', max_iter=10000))\n])\n\ndt_pipe = Pipeline([\n    ('dt', DecisionTreeClassifier())\n])\n\nrf_pipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('rf', RandomForestClassifier())\n])\n\n\n\n\nShow the code\n\ndef build_classifier(pipeline, param_grid, X,y, outer_cv=None, inner_cv=None):\n    \"\"\"\n    This builds CRC classifier using nested cross-validation and print hyperparameters of the model.\n\n    Args:\n    -----\n        model (sklearn model): machine learning model object\n        \n        param_grid (dict): parameters grid to search optimized parameters\n        \n        X (dataframe): pandas dataframe of dataset\n        \n        y (list): target class\n        \n        inner_cv_scoring (int): number of folds for inner cross-validation\n        \n        outer_cv_scoring (int): number of folds for outer cross-validation\n        \n\n    Returns:\n    -----\n        classifier (sklearn trained model)\n    \"\"\"\n    \n    # Outer validation: 10-fold cross-validation\n    if outer_cv is None:\n        outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n    # Inner validation: 5-fold cross-validation for hyperparameter tuning\n    if inner_cv is None:\n        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    \n    aucs = []\n    best_performance = 0\n    best_estimator = None\n\n    X = X.reset_index(drop=True)\n    y = y.reset_index(drop=True)\n\n    # Nested cross-validation\n    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n        X_train, X_test = X.iloc[train_idx,:], X.iloc[test_idx,:]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        # Perform GridSearchCV with 10-fold cross-validation\n        grid_search = GridSearchCV(\n            pipeline, param_grid, scoring='roc_auc_ovr', cv=inner_cv, n_jobs=-1\n        )\n\n        # searching parameters first\n        grid_search.fit(X_train, y_train)\n\n        # Best model and hyperparameter\n        best_model = grid_search.best_estimator_\n        #print(f\"Best Hyperparameter: {grid_search.best_params_}\")\n\n        # Fit the model and predict probabilities\n        best_model.fit(X_train, y_train)\n        y_prob = best_model.predict_proba(X_test)\n\n\n        fpr, tpr, _ = roc_curve(y_test, y_prob[:,1])\n        roc_auc = auc(fpr, tpr)\n        \n        if roc_auc &gt; best_performance:\n            best_estimator = best_model\n            best_performance = roc_auc\n        \n        aucs.append(roc_auc)\n        #print(f\"  performance={roc_auc:.2f}\")\n\n    return best_estimator, best_performance, np.std(aucs)\n\n\n\n\nShow the code\n\nclf, per, st = build_classifier(lasso_pipe, lasso_param_grid, X_final, y)\n\nprint(f' Best estimator (performance={per:.2f}):')\nprint(clf)\n\n\n Best estimator (performance=0.96):\nPipeline(steps=[('scaler', StandardScaler()),\n                ('lasso',\n                 LogisticRegression(C=0.1, max_iter=10000, penalty='l1',\n                                    solver='liblinear'))])\n\n\n\n\nShow the code\nclf, per, st = build_classifier(dt_pipe, dt_param_grid, X_final, y)\n\nprint(f' Best dt estimator (performance={per:.2f}±{st:.2f}):')\nprint(clf)\n\n\n Best dt estimator (performance=0.89±0.09):\nPipeline(steps=[('dt',\n                 DecisionTreeClassifier(criterion='entropy', max_depth=4,\n                                        min_samples_split=10))])\n\n\n\n\nShow the code\nclf, per, st = build_classifier(rf_pipe, rf_param_grid, X_final, y)\n\nprint(f' Best rf estimator (performance={per:.2f}±{st:.2f}):')\nprint(clf)\n\n\n Best rf estimator (performance=0.96±0.07):\nPipeline(steps=[('scaler', StandardScaler()),\n                ('rf', RandomForestClassifier(max_depth=6, n_estimators=200))])",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter6.html#best-models",
    "href": "chapters/Analysis/chapter6.html#best-models",
    "title": "Building a CRC classifier",
    "section": "Best models",
    "text": "Best models\nWe selected three different machine learning models based on their performance on entire dataset. The models were evluated in a nested cross-validation. The outer cross-validation (10-fold) was used for model performance while the inner cross-validation (5-fold) was used for hyperparameter tuning.\n\nLogisticRegression(C=.1) .96 AUC\nDecisionTreeClassifier(criterion=entropy, max_depth=4, min_samples_split=10) AUC= 0.83±0.09\nRandomForestClassifier(max_depth=6, n_estimators=200) AUC = 0.93±0.07\n\n\n\nShow the code\nlg = LogisticRegression(C=.1, penalty='l1',solver='liblinear')\ndt = DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=10) \nrf = RandomForestClassifier(max_depth=6, n_estimators=200)",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/chapter6.html#model-performance-of-selected-model-using-leaveonegroupout",
    "href": "chapters/Analysis/chapter6.html#model-performance-of-selected-model-using-leaveonegroupout",
    "title": "Building a CRC classifier",
    "section": "Model performance of selected model using LeaveOneGroupOut",
    "text": "Model performance of selected model using LeaveOneGroupOut\nWe will now assess these models using leavel-one-group-out strategy. In this strategy, we will keep a cohort for testing while other cohorts datasets will be used for training. This will give us a better idea about our models’ performance.\n\n\nShow the code\nfrom sklearn.model_selection import LeaveOneGroupOut\n\ndef perform_logo(estimator, X, y, groups,title=''):\n    \"\"\"\n    This function performs Leave-one-group-out evaluation of the model.\n    \"\"\"\n    \n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    aucs = []\n    best_performance = 0\n    best_estimator = None\n    \n    logo = LeaveOneGroupOut()\n    \n    # Nested cross-validation\n    for fold, (train_idx, test_idx) in enumerate(logo.split(X, y,groups)):\n        X_train, X_test = X.iloc[train_idx,:], X.iloc[test_idx,:]\n        y_train, y_test = y[train_idx], y[test_idx]\n    \n        estimator.fit(X_train, y_train)\n        \n        y_prob = estimator.predict_proba(X_test)\n\n        fpr, tpr, _ = roc_curve(y_test, y_prob[:,1])\n        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        roc_auc = auc(fpr, tpr)\n\n        aucs.append(roc_auc)\n        \n    # Average AUC across classes\n    mean_auc = np.mean(aucs)\n\n    # Plot the mean ROC curve\n    # Calculate mean and standard deviation of TPRs\n    mean_tpr = np.mean(tprs, axis=0)\n    std_tpr = np.std(tprs, axis=0)\n    \n    # Plot the mean ROC curve\n    plt.plot(mean_fpr, mean_tpr, color='blue', linestyle='--', lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')\n\n    # Fill the area between the mean TPR and ±1 standard deviation\n    plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, color='blue', alpha=0.2, label='± 1 Std. Dev.')\n\n    # Plot the random chance line\n    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', lw=2, label='Chance')\n\n    # Finalize the plot\n    plt.title(title)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc='lower right')\n    #plt.grid(alpha=0.3)\n    plt.show()\n\n\n\n\nShow the code\ntrain_X, train_y = prepare_dataset(train_X_df)\ntest_X, test_y = prepare_dataset(test_X_df)\n\nfull_pipeline.fit(train_X, train_y)\n\n\nPipeline(steps=[('combined_part',\n                 FeatureUnion(transformer_list=[('abun_part',\n                                                 Pipeline(steps=[('abundance',\n                                                                  ColumnSelector(variables=['OTU_0',\n                                                                                            'OTU_1',\n                                                                                            'OTU_2',\n                                                                                            'OTU_3',\n                                                                                            'OTU_4',\n                                                                                            'OTU_5',\n                                                                                            'OTU_6',\n                                                                                            'OTU_7',\n                                                                                            'OTU_8',\n                                                                                            'OTU_9',\n                                                                                            'OTU_10',\n                                                                                            'OTU_11',\n                                                                                            'OTU_12',\n                                                                                            'OTU_13',\n                                                                                            'OTU_14',\n                                                                                            'OTU_15',\n                                                                                            'OTU_16',\n                                                                                            'OTU_17',\n                                                                                            'OTU_18',\n                                                                                            'OTU_19',\n                                                                                            'OTU_20',\n                                                                                            'OTU_21',\n                                                                                            'OTU_22',\n                                                                                            'OTU_23',\n                                                                                            'OTU_24',\n                                                                                            'OT...\n                 ColumnSelector(variables=['OTU_16', 'OTU_28', 'OTU_108',\n                                           'OTU_111', 'OTU_144', 'OTU_153',\n                                           'OTU_157', 'OTU_158', 'OTU_161',\n                                           'OTU_178', 'OTU_218', 'OTU_348',\n                                           'OTU_353', 'OTU_367', 'OTU_372',\n                                           'OTU_375', 'OTU_378', 'OTU_381',\n                                           'OTU_388', 'OTU_446', 'OTU_464',\n                                           'OTU_465', 'OTU_466', 'OTU_468',\n                                           'OTU_470', 'OTU_503', 'OTU_535',\n                                           'OTU_544', 'OTU_546', 'OTU_547', ...])),\n                ('train',\n                 LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  Pipeline?Documentation for PipelineiFittedPipeline(steps=[('combined_part',\n                 FeatureUnion(transformer_list=[('abun_part',\n                                                 Pipeline(steps=[('abundance',\n                                                                  ColumnSelector(variables=['OTU_0',\n                                                                                            'OTU_1',\n                                                                                            'OTU_2',\n                                                                                            'OTU_3',\n                                                                                            'OTU_4',\n                                                                                            'OTU_5',\n                                                                                            'OTU_6',\n                                                                                            'OTU_7',\n                                                                                            'OTU_8',\n                                                                                            'OTU_9',\n                                                                                            'OTU_10',\n                                                                                            'OTU_11',\n                                                                                            'OTU_12',\n                                                                                            'OTU_13',\n                                                                                            'OTU_14',\n                                                                                            'OTU_15',\n                                                                                            'OTU_16',\n                                                                                            'OTU_17',\n                                                                                            'OTU_18',\n                                                                                            'OTU_19',\n                                                                                            'OTU_20',\n                                                                                            'OTU_21',\n                                                                                            'OTU_22',\n                                                                                            'OTU_23',\n                                                                                            'OTU_24',\n                                                                                            'OT...\n                 ColumnSelector(variables=['OTU_16', 'OTU_28', 'OTU_108',\n                                           'OTU_111', 'OTU_144', 'OTU_153',\n                                           'OTU_157', 'OTU_158', 'OTU_161',\n                                           'OTU_178', 'OTU_218', 'OTU_348',\n                                           'OTU_353', 'OTU_367', 'OTU_372',\n                                           'OTU_375', 'OTU_378', 'OTU_381',\n                                           'OTU_388', 'OTU_446', 'OTU_464',\n                                           'OTU_465', 'OTU_466', 'OTU_468',\n                                           'OTU_470', 'OTU_503', 'OTU_535',\n                                           'OTU_544', 'OTU_546', 'OTU_547', ...])),\n                ('train',\n                 LogisticRegression(C=0.1, penalty='l1', solver='liblinear'))])  combined_part: FeatureUnion?Documentation for combined_part: FeatureUnionFeatureUnion(transformer_list=[('abun_part',\n                                Pipeline(steps=[('abundance',\n                                                 ColumnSelector(variables=['OTU_0',\n                                                                           'OTU_1',\n                                                                           'OTU_2',\n                                                                           'OTU_3',\n                                                                           'OTU_4',\n                                                                           'OTU_5',\n                                                                           'OTU_6',\n                                                                           'OTU_7',\n                                                                           'OTU_8',\n                                                                           'OTU_9',\n                                                                           'OTU_10',\n                                                                           'OTU_11',\n                                                                           'OTU_12',\n                                                                           'OTU_13',\n                                                                           'OTU_14',\n                                                                           'OTU_15',\n                                                                           'OTU_16',\n                                                                           'OTU_17',\n                                                                           'OTU_18',\n                                                                           'OTU_19',\n                                                                           'OTU_20',\n                                                                           'OTU_21',\n                                                                           'OTU_22',\n                                                                           'OTU_23',\n                                                                           'OTU_24',\n                                                                           'OTU_25',\n                                                                           'OTU_26',\n                                                                           'OTU_27',\n                                                                           'OTU_28',...\n                               ('pca_part',\n                                Pipeline(steps=[('abundance',\n                                                 ColumnSelector(variables=['OTU_0',\n                                                                           'OTU_1',\n                                                                           'OTU_2',\n                                                                           'OTU_3',\n                                                                           'OTU_4',\n                                                                           'OTU_5',\n                                                                           'OTU_6',\n                                                                           'OTU_7',\n                                                                           'OTU_8',\n                                                                           'OTU_9',\n                                                                           'OTU_10',\n                                                                           'OTU_11',\n                                                                           'OTU_12',\n                                                                           'OTU_13',\n                                                                           'OTU_14',\n                                                                           'OTU_15',\n                                                                           'OTU_16',\n                                                                           'OTU_17',\n                                                                           'OTU_18',\n                                                                           'OTU_19',\n                                                                           'OTU_20',\n                                                                           'OTU_21',\n                                                                           'OTU_22',\n                                                                           'OTU_23',\n                                                                           'OTU_24',\n                                                                           'OTU_25',\n                                                                           'OTU_26',\n                                                                           'OTU_27',\n                                                                           'OTU_28',\n                                                                           'OTU_29', ...])),\n                                                ('pca', PCA())]))]) abun_partColumnSelectorColumnSelector(variables=['OTU_0', 'OTU_1', 'OTU_2', 'OTU_3', 'OTU_4', 'OTU_5',\n                          'OTU_6', 'OTU_7', 'OTU_8', 'OTU_9', 'OTU_10',\n                          'OTU_11', 'OTU_12', 'OTU_13', 'OTU_14', 'OTU_15',\n                          'OTU_16', 'OTU_17', 'OTU_18', 'OTU_19', 'OTU_20',\n                          'OTU_21', 'OTU_22', 'OTU_23', 'OTU_24', 'OTU_25',\n                          'OTU_26', 'OTU_27', 'OTU_28', 'OTU_29', ...]) RelativeAbundanceFilterRelativeAbundanceFilter(threshold=0.001) VarianceThresholdFilterVarianceThresholdFilter()  FunctionTransformer?Documentation for FunctionTransformerFunctionTransformer(func=&lt;function log at 0x30fbb2a20&gt;)  MinMaxScaler?Documentation for MinMaxScalerMinMaxScaler() alpha_partColumnSelectorColumnSelector(variables=['OTU_0', 'OTU_1', 'OTU_2', 'OTU_3', 'OTU_4', 'OTU_5',\n                          'OTU_6', 'OTU_7', 'OTU_8', 'OTU_9', 'OTU_10',\n                          'OTU_11', 'OTU_12', 'OTU_13', 'OTU_14', 'OTU_15',\n                          'OTU_16', 'OTU_17', 'OTU_18', 'OTU_19', 'OTU_20',\n                          'OTU_21', 'OTU_22', 'OTU_23', 'OTU_24', 'OTU_25',\n                          'OTU_26', 'OTU_27', 'OTU_28', 'OTU_29', ...]) AlphaFeatureExtenderAlphaFeatureExtender() meta_partColumnSelectorColumnSelector(variables=['age', 'BMI', 'gender']) MetaDataImputerMetaDataImputer() pca_partColumnSelectorColumnSelector(variables=['OTU_0', 'OTU_1', 'OTU_2', 'OTU_3', 'OTU_4', 'OTU_5',\n                          'OTU_6', 'OTU_7', 'OTU_8', 'OTU_9', 'OTU_10',\n                          'OTU_11', 'OTU_12', 'OTU_13', 'OTU_14', 'OTU_15',\n                          'OTU_16', 'OTU_17', 'OTU_18', 'OTU_19', 'OTU_20',\n                          'OTU_21', 'OTU_22', 'OTU_23', 'OTU_24', 'OTU_25',\n                          'OTU_26', 'OTU_27', 'OTU_28', 'OTU_29', ...])  PCA?Documentation for PCAPCA() ColumnSelectorColumnSelector(variables=['OTU_16', 'OTU_28', 'OTU_108', 'OTU_111', 'OTU_144',\n                          'OTU_153', 'OTU_157', 'OTU_158', 'OTU_161', 'OTU_178',\n                          'OTU_218', 'OTU_348', 'OTU_353', 'OTU_367', 'OTU_372',\n                          'OTU_375', 'OTU_378', 'OTU_381', 'OTU_388', 'OTU_446',\n                          'OTU_464', 'OTU_465', 'OTU_466', 'OTU_468', 'OTU_470',\n                          'OTU_503', 'OTU_535', 'OTU_544', 'OTU_546', 'OTU_547', ...])  LogisticRegression?Documentation for LogisticRegressionLogisticRegression(C=0.1, penalty='l1', solver='liblinear') \n\n\n\nShow the code\nfrom sklearn import metrics\n\ntest_y_proba = full_pipeline.predict_proba(test_X)[:,1]\n\nmetrics.RocCurveDisplay.from_estimator(full_pipeline, train_X, train_y)\nmetrics.RocCurveDisplay.from_estimator(full_pipeline, test_X, test_y)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Train\n\n\n\n\n\n\n\n\n\n\n\n(b) Test\n\n\n\n\n\n\n\nFigure 3: Lasso model performance\n\n\n\n\n\nShow the code\ngroups = train_X_df['groups'].map({'france':0,'austria':1,'germany':2,'italy':3})\ngroups = groups.reset_index(drop=True)\n\n\n\nShow the code\n#train_X, train_y, transformer = prepare_dataset(train_X_df)\n\nperform_logo(full_pipeline, X, y, groups,'Logistic regression across test cohorts')\n#perform_logo(dt, train_X,train_y, groups,'Decision tree across cohorts (training)')\n#perform_logo(rf, train_X,train_y, groups,'Random forest across cohorts (training)')\n\n\n\n\n\n\n\n\n\n\nFigure 4: Logistic regression\n\n\n\n\n\nModel training performance across cohorts (LOGO)\n\n\n\n\nModel performance on test data from different cohorts\nNow, we will evaluate our trained model which were trained using data from different cohorts. This evaluation is done on a seperate dataset which was kept aside earlier.\n\n\nShow the code\nlg.fit(train_X, train_y)\ndt.fit(train_X, train_y)\nrf.fit(train_X, train_y)\n\n\nRandomForestClassifier(max_depth=6, n_estimators=200)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(max_depth=6, n_estimators=200) \n\n\n\n\nShow the code\ntrain_pre_X, train_y = prepare_dataset(train_X_df)\n\ncombined_features.fit(train_pre_X, train_y)\n\ntrain_X = combined_features.transform(train_pre_X)\n\n\n\n\nShow the code\nfrom sklearn.metrics import RocCurveDisplay\n\nestimator = lg\n\nmean_fpr = np.linspace(0, 1, 100)\ncn_test_set = test_X_df.loc[test_X_df['country']=='ITA',:]\ncn_test_pre_X, cn_test_y = prepare_dataset(cn_test_set)\ncn_test_X = combined_features.transform(cn_test_pre_X)\ny_prob = estimator.predict_proba(cn_test_X)\nfpr, tpr, _ = roc_curve(cn_test_y, y_prob[:,1])\nroc_auc = auc(fpr, tpr)\ninterp_tpr = np.interp(mean_fpr, fpr, tpr)\ninterp_tpr[0] = 0.0\n\n# Plot the mean ROC curve\nplt.plot(mean_fpr, interp_tpr, color='blue', linestyle='--', lw=2, label=f'French (AUC = {roc_auc:.2f})')\nplt.legend()",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapters/Analysis/generalizability.html",
    "href": "chapters/Analysis/generalizability.html",
    "title": "Generalizability evaluation",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "./Analysis/Analysis_intro.qmd",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Generalizability evaluation</span>"
    ]
  }
]