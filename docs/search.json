[
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "6  Data loading",
    "section": "",
    "text": "6.1 Age, BMI and gender distribution\nFigure 6.1 shows distributions of age, BMI, and gender of patients across different study conditions: control, adenoma, and CRC.",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data loading</span>"
    ]
  },
  {
    "objectID": "chapter1.html#age-bmi-and-gender-distribution",
    "href": "chapter1.html#age-bmi-and-gender-distribution",
    "title": "6  Data loading",
    "section": "",
    "text": "CRC patients are slightly older than control cases.\nThere is an increase in BMI for adenoma compared to control cases.\nThere are more males with CRC than females.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Age distribution\n\n\n\n\n\n\n\n\n\n\n\n(b) BMI distribution\n\n\n\n\n\n\n\n\n\n\n\n(c) Gender distribution\n\n\n\n\n\n\n\nFigure 6.1: Distribution across study conditions\n\n\n\n\n\n\n\nZeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766.",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Data loading</span>"
    ]
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "7  Species filtering",
    "section": "",
    "text": "7.1 References",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Species filtering</span>"
    ]
  },
  {
    "objectID": "chapter2.html#references",
    "href": "chapter2.html#references",
    "title": "7  Species filtering",
    "section": "",
    "text": "Zeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766.",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Species filtering</span>"
    ]
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "8  Feature transformation",
    "section": "",
    "text": "8.1 References",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Feature transformation</span>"
    ]
  },
  {
    "objectID": "chapter3.html#references",
    "href": "chapter3.html#references",
    "title": "8  Feature transformation",
    "section": "",
    "text": "Zeller, Georg, Julien Tap, Anita Y Voigt, Shinichi Sunagawa, Jens Roat Kultima, Paul I Costea, Aurélien Amiot, et al. 2014. “Potential of Fecal Microbiota for Early-Stage Detection of Colorectal Cancer.” Molecular Systems Biology 10 (11): 766.",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Feature transformation</span>"
    ]
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "9  Principal Component Analysis",
    "section": "",
    "text": "Principal Component Analysis (PCA) is a powerful dimensionality reduction technique that reduces the number of features (dimensions) while retaining the maximum variance in the data. By capturing the most important patterns in fewer components, PCA enhances data visualization and simplifies analysis.\nWe will now apply PCA to our filtered dataset and plot the resulting components to explore potential associations with study conditions. We will apply this transformation on filtered species. The code below perform that step.\n\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# log transformed data from the previous step\ndf = microbiome_log\n\n# Step 1: Standardize the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df)\n\n# Step 2: Apply PCA (reduce to 10 components for visualization)\npca = PCA(n_components=10)\npca_result = pca.fit_transform(scaled_data)\n\n# Step 3: Create a DataFrame for PCA results\npca_df = pd.DataFrame(pca_result, columns=['PCA1', 'PCA2','PCA3', 'PCA4','PCA5',\n                                           'PCA6','PCA7', 'PCA8', 'PCA9', 'PCA10'])\n\npca_df['study_condition'] = zeller_db['study_condition'].values\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) PCA1\n\n\n\n\n\n\n\n\n\n\n\n(b) PCA2\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) PCA3\n\n\n\n\n\n\n\n\n\n\n\n(d) PCA4\n\n\n\n\n\n\n\n\n\n\n\n\n\n(e) PCA5\n\n\n\n\n\n\n\n\n\n\n\n(f) PCA6\n\n\n\n\n\n\n\n\n\n\n\n\n\n(g) PCA7\n\n\n\n\n\n\n\n\n\n\n\n(h) PCA8\n\n\n\n\n\n\n\n\n\n\n\n\n\n(i) PCA9\n\n\n\n\n\n\n\n\n\n\n\n(j) PCA910\n\n\n\n\n\n\n\nFigure 9.1: Principal components\n\n\n\nFigure 9.2 below shows the cumulative variance captured by 10 principal components, i.e. 26%.\n\n\n\n\n\n\n\n\nFigure 9.2: Cumulative Explained Variance by Principal Components\n\n\n\n\n\nWe will utilize PCA components in our modeling phase to build a CRC detection system.",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Principal Component Analysis</span>"
    ]
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "10  Statistical analysis",
    "section": "",
    "text": "10.1 Association between host’s characteristics and CRC\nWe will now investigate for statistical differences among target classes in terms of patient demographics.",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "chapter5.html#association-between-hosts-characteristics-and-crc",
    "href": "chapter5.html#association-between-hosts-characteristics-and-crc",
    "title": "10  Statistical analysis",
    "section": "",
    "text": "10.1.1 Age, BMI, and CRC\nWe start by investigating statistical significance of differences between age of control and CRC casese. For this test, we use Mann-Whitney test to investigate statistical significance of differences.\nThe below figures show that **there are statistical significant differences between control and CRC groups.\n\n\n\n\n\n\nAge\n\n\n\nWe found statistical differences in age of control and CRC group.\n\n\n\n\nShow the code\nfrom skbio.diversity import alpha\nfrom statannot import add_stat_annotation\n\n# functions to process the data and prepare in a format supporting phyloseq analysis\ndef get_sample_table(df):\n    df = df[metadata_colnames]\n    df = df.set_index('subjectID')\n    df.drop(['dataset_name','sampleID'],axis=1,inplace=True)\n    return df\n\n\ndef get_otu_taxa_table(df):\n    \"\"\"\n    This function returns otu table that contains relative abundance of species \n    where columns are species and rows are cases.\n    \n    Args:\n        df (dataframe): Dataframe of realtive abundance and metadata\n    \"\"\"\n    df = df[bacteria_colnames + ['subjectID']]\n    df.columns = [\"OTU_{}\".format(str(ind)) for ind, col in enumerate(bacteria_colnames)] + ['subjectID']\n    df.index = df['subjectID']\n    taxa_table = get_taxa_table(bacteria_colnames)\n    return df, taxa_table\n\n\ndef get_taxa_table(list_of_otus):\n    \"\"\"\n    This function parse all present microbial species at different heirarchy levels, e.g., class, order, phylum.\n    \n    \"\"\"\n    otu = 0\n    mapping = {}\n    taxa_cols = ['kingdom','phylum','class','order','family','genus','species']\n    df = pd.DataFrame(columns=taxa_cols)\n    otu_mapping = {}\n    otu_ids = []\n    for ind, otu in enumerate(list_of_otus):\n        tmp = {}\n        for col in taxa_cols:\n            tmp[col] = get_specific_label(otu, col)\n        tmp_df = pd.DataFrame([tmp])\n        df = pd.concat([df,tmp_df], ignore_index=[0])\n\n        otu_id = \"OTU_{}\".format(str(ind))\n        otu_mapping[otu] = otu_id\n        otu_ids.append(otu)\n\n    df['OTU'] = ['OTU_{}'.format(str(ind)) for ind in df.index]\n    df = df.set_index('OTU')\n    return df\n\n\ndef get_specific_label(l, t):\n    \"\"\"\n    This function parse the taxonomic assignment lable and fetch the specified information (e.g., kingdom, family)\n\n    Args:\n        l (str): string of taxonomy\n        t (str): string specifying the requested information (e.g., kingdom, family, genus, etc.)\n\n    Returns:\n        str: requested heirarichal info \n    \"\"\"\n    taxa_order = {'kingdom':0,'phylum':1,'class':2,'order':3,'family':4,'genus':5,'species':6}\n\n    try:\n        specific_label = l.split('|')[taxa_order[t]]\n\n        return specific_label.strip().split('__')[1]\n    except:\n        return 'Unknown'\n\n    \ndef get_otu_detail(taxa_table, otu_label, rank):\n    return taxa_table[otu_label][rank]\n\n\ndef aggregate_by_taxonomy(otu_table, taxa_table, taxa_rank):\n    \"\"\"\n    This function aggregates data based on specified \n    taxa rank (e.g., kingdom, class, order, phylum, genus, species).\n    \"\"\"\n    unique_values = (taxa_table[taxa_rank].unique())\n\n    # mapping for otus to unique value of chosen taxa rank\n    taxa_to_otu = {}\n\n    # prepare the mapping\n    for unique_value in unique_values:\n        tdf = taxa_table.loc[taxa_table[taxa_rank] == unique_value, :]\n        otus = tdf.index.to_list()   \n        taxa_to_otu[unique_value.strip()] = otus\n    \n    # create a dictionary for formulating expressions\n    taxa_to_exp = {}\n\n    for key in taxa_to_otu.keys():\n        taxa_to_exp[key] = '{} = 0.000001'.format(key)\n        for otu in taxa_to_otu[key]:\n            taxa_to_exp[key] += ' + ' + otu\n            otu_table[otu] = pd.to_numeric(otu_table[otu], errors='coerce')\n\n    agg_df = otu_table\n    \n    for key, expr in taxa_to_exp.items():\n        agg_df[key] = 0\n        agg_df = agg_df.eval(expr, engine='python')\n        \n    agg_df = agg_df[list(unique_values)]\n    \n    return agg_df   \n\n\ndef extend_with_alpha(df, metadata_features):\n    \"\"\"\n    This function extends the dataframe with alpha diversity measures.\n    \n    Args:\n        df: dataframe\n        \n        metadata_features: list of metadata feature names\n        \n    Returns:\n        dataframe: extended dataframe with alpha diversity features\n    \"\"\"\n    diversity_measures = pd.DataFrame()\n\n    alpha_diversity_metrics = [\n        \"chao1\",\n        \"shannon\",\n        \"simpson\",\n        \"simpson_e\",\n        \"fisher_alpha\",\n        \"berger_parker\"\n    ]\n\n    shannon_diversity = df.apply(lambda x: alpha.shannon(x), axis=1)\n    chao1_diversity   = df.apply(lambda x: alpha.chao1(x), axis=1)\n    simpson_diversity   = df.apply(lambda x: alpha.simpson(x), axis=1)\n    simpson_e_diversity   = df.apply(lambda x: alpha.simpson_e(x), axis=1)\n    fisher_diversity   = df.apply(lambda x: alpha.fisher_alpha(x), axis=1)\n    berger_parker_diversity   = df.apply(lambda x: alpha.berger_parker_d(x), axis=1)\n\n    diversity_measures['shannon'] = shannon_diversity\n    diversity_measures['chao1'] = chao1_diversity\n    diversity_measures['simpson'] = simpson_diversity\n    diversity_measures['simpson_e'] = simpson_e_diversity\n    diversity_measures['fisher_alpha'] = fisher_diversity\n    diversity_measures['berger_parker'] = berger_parker_diversity\n    \n    X_alpha = diversity_measures.reset_index().drop(['subjectID'], axis=1)\n    X_extended = pd.concat([metadata_features,X_alpha],axis=1)\n    \n    return X_extended\n\n# convert data tables into otu and taxa table\notu_table, taxa_table = get_otu_taxa_table(zeller_db)\n\n# aggregating data at higher levels\nphylum_agg = aggregate_by_taxonomy(otu_table, taxa_table, 'phylum')\ngenus_agg = aggregate_by_taxonomy(otu_table, taxa_table, 'genus')\norder_agg = aggregate_by_taxonomy(otu_table, taxa_table, 'order')\n\ncolor_palette = {'control':'green',\n                'adenoma':'orange',\n                'CRC':'#c80000'}\n\norder = ['control', 'adenoma', 'CRC']\nx = 'study_condition'\n\npairs = [\n    ('control','adenoma'),\n    ('control','CRC'),\n    ('adenoma','CRC'),\n]\n\nmetadata = zeller_db[metadata_colnames]\n\n# changing data type of age and BMI\nmetadata['age'] = pd.to_numeric(metadata.age, errors='coerce')\nmetadata['BMI'] = pd.to_numeric(metadata.BMI, errors='coerce')\n\n\n\nShow the code\n# plotting distribution\n\nfor ind, y in enumerate(['age','BMI']):\n    plt.figure()\n    ax = sns.boxplot(data=metadata, y=y,x=x, palette=color_palette, order= order)\n    #annot = Annotator(ax, pairs=pairs, data=metadata, x=x, y=y, hue=x, hue_order=order, order=order)\n    ax, test_results = add_stat_annotation(ax, box_pairs=pairs, data=metadata, x=x, y=y,\n                                           hue_order=order, order=order,\n                                           test='Mann-Whitney', text_format='star',comparisons_correction=None, \n                                           loc='inside', verbose=False)\n\n    plt.title(f'{y.upper()}')\n\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n(a) Age\n\n\n\n\n\n\n\n\n\n\n\n(b) BMI\n\n\n\n\n\n\n\nFigure 10.1: Age/BMI distribution\n\n\n\n\n\n10.1.2 Gender and CRC\nWe now check the distribution of gender across different target groups (i.e., control, adenoma, CRC). We employ the Chi-squared test to investigate the statistical significance of differences in gender distribution across different groups.\nFigure 10.2 shows the frequency count of males/females across control, adenoma, and CRC groups. The differences were found to be statistically significant (p-value &lt; .05).\n\n\n\n\n\n\nGender\n\n\n\nWe also found statistical differences in gender between control, adenoma and CRC group.\n\n\n\n\nShow the code\nfrom scipy.stats import chi2_contingency\n\n# Create a contingency table\ncontingency_table = pd.crosstab(metadata['gender'], metadata['study_condition'])\n\n# Apply Fisher's Exact Test\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\n\n# new figure\nplt.figure()\n\n# plot frequency plot\nsns.countplot(data=metadata, x='study_condition',hue='gender')\n\n# add p-value\nplt.text(0.5, 32, f'$X^2 test$ p-value: {p_value:.4f}', fontsize=12, color='blue')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10.2: Gender distribution",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "chapter5.html#association-between-hosts-characteristics-and-species-abundance",
    "href": "chapter5.html#association-between-hosts-characteristics-and-species-abundance",
    "title": "10  Statistical analysis",
    "section": "10.2 Association between host’s characteristics and species abundance",
    "text": "10.2 Association between host’s characteristics and species abundance\nWe will now explore for association between host’s characteristics (e.g., age, BMI) and species relative abundace. Figure 10.3 below shows correlation between host’s characteristics and phylum level abundance data.\n\nFirnicutes, Spirochaetes, and Verrucomicrobia phylum levels are negatively correlated with age. That means as a person gets older these three phylum levels tend to get decreased.\nIn case of BMI, Spirochaetes, Firnicutes, Deferribacteres, Bacteroidetes, and Actinobacteria are found negatively correlated. That implies an increase in BMI (which could be taken as an indication of obesity) is associated with decrease in those phylums.\n\n\nShow the code\n# plotting distribution\n\ndef extend_abundance_metadata(df,meta):\n    \"\"\"\n    This function extends abundance data with metadata information.\n    \n    Args:\n        df (DataFrame): relative abundance data\n        meta (DataFrame): host's characteristics\n        \n    Returns:\n        DataFrame\n    \"\"\"\n    return pd.concat([df,meta],axis=1)\n\n# relative abundance aggregation at \n#family_abundance = aggregate_by_taxonomy(otu_table, taxa_table, 'family')\ngenus_abundance = aggregate_by_taxonomy(otu_table, taxa_table, 'genus')\nphylum_abundance = aggregate_by_taxonomy(otu_table, taxa_table, 'phylum')\nmetadata_ = metadata.set_index(metadata['subjectID'])\n\n# plot age correlation plot\nphylum_metadata = extend_abundance_metadata(phylum_abundance,metadata_[['age']])\nphylum_corr = phylum_metadata.corr()\nplt.figure(figsize=(7,10))\ndata_plot = phylum_corr['age'].drop('age')\nbars = plt.barh(data_plot.index, data_plot, color=np.where(data_plot &gt; 0, 'green', 'red'))\nplt.xlim([-.5,.5])\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.title('Age')\nplt.show()\n\n# plot bmi correlation plot\nphylum_metadata = extend_abundance_metadata(phylum_abundance,metadata_[['BMI']])\nphylum_corr = phylum_metadata.corr()\nplt.figure(figsize=(7,10))\ndata_plot = phylum_corr['BMI'].drop('BMI')\nbars = plt.barh(data_plot.index, data_plot, color=np.where(data_plot &gt; 0, 'green', 'red'))\nplt.xlim([-.5,.5])\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.title('BMI')\nplt.show()\n    \nphylum_metadata_ = phylum_metadata.copy()\nphylum_metadata_['study_condition'] = metadata_['study_condition'].apply(\n    lambda x: 'malign' if x == 'CRC' else 'benign')\n\ndf = phylum_metadata_.melt(id_vars='study_condition',value_vars=phylum_agg.columns)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Correlation between age and phylum level abundance\n\n\n\n\n\n\n\n\n\n\n\n(b) Correlation between BMI and phylum level abundance\n\n\n\n\n\n\n\nFigure 10.3: Correlation with phylum abundace\n\n\n\nFigure 10.4 (c) below shows differences in microbial composition in terms of phylum level abundance among benign and malign tumors. We can notice three phylums differ among benign and malign tumor groups. Those phylums are Proteobacteria, Firnicutes, and Bacteroidetes.\n\nWe combined control with adenoma to create benign tumor class, and CRC class renamed as malign tumor.\n\n\n\nShow the code\nplt.figure(figsize=(7,10))\nsns.boxplot(data=df, y='variable',x='value', order=list(data_plot.index)[::-1], hue='study_condition', palette={'benign':'green','malign':'red'})\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10.4: CRC and phylum abundace",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "chapter5.html#references",
    "href": "chapter5.html#references",
    "title": "7  Statistical analysis",
    "section": "7.4 References",
    "text": "7.4 References",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "generalizability.html",
    "href": "generalizability.html",
    "title": "12  Generalizability evaluation",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Generalizability evaluation</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Gut microbiome analysis for CRC detection: from raw sequences to machine learning models",
    "section": "",
    "text": "Preface\nThis book is a result of the author’s reflection of his learning of the field of bioinformatics. This book is created to bundle up all relevant learning materials and tutorials for gut microbiome analysis with a particular focus on early CRC detection.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Colorectal Cancer Detection",
    "section": "",
    "text": "Analysis pipeline\nFor our analysis, we will follow the steps depicted in the figure below. These steps are taken from refer to analysis pipeline of the same study.\nflowchart LR\n  A(Data Loading) --&gt; B(Species filtering)\n  B --&gt; C(Feature transformation)\n  C --&gt; D(Statistical analysis)\n  D --&gt; E(Model building)\n  F --&gt; E(Model building)\n  G --&gt; E(Model building)\n  C --&gt; F(Enterotype inference)\n  C --&gt; G(PCA)\n  E --&gt; H(Generalizability evaluation)",
    "crumbs": [
      "Colorectal Cancer Detection"
    ]
  },
  {
    "objectID": "intro.html#analysis-pipeline",
    "href": "intro.html#analysis-pipeline",
    "title": "Colorectal Cancer Detection",
    "section": "",
    "text": "Data Loading: The process begins by loading the dataset, which contains relative abundance data and metadata.\nSpecies Filtering: In this step, microbial species are filtered to retain only those relevant for analysis, improving data quality and reducing noise.\nFeature Transformation: The selected features undergo log-transformation to prepare the data for statistical analysis and downstream modeling.\nStatistical Analysis: Various statistical techniques are applied to identify significant patterns and associations within the data.\nEnterotype Inference: Enterotypes, or gut microbial community types, which were consistently found across different cohorts, are inferred to explore microbiome structure and its potential link to CRC.\nPrincipal Component Analysis (PCA): PCA reduces data dimensionality, capturing key variance and enhancing model interpretability.\nModel Building: This central step integrates insights from statistical analysis, enterotype inference, and PCA to develop the CRC classifier.\nGeneralizability Evaluation: Finally, the model’s robustness and generalizability are evaluated on datasets from different cohorts.",
    "crumbs": [
      "Colorectal Cancer Detection"
    ]
  },
  {
    "objectID": "intro.html#references",
    "href": "intro.html#references",
    "title": "Colorectal Cancer Detection",
    "section": "References",
    "text": "References\n\n\n\n\nThomas, Andrew Maltez, Paolo Manghi, Francesco Asnicar, Edoardo Pasolli, Federica Armanini, Moreno Zolfo, Francesco Beghini, et al. 2019. “Metagenomic Analysis of Colorectal Cancer Datasets Identifies Cross-Cohort Microbial Diagnostic Signatures and a Link with Choline Degradation.” Nature Medicine 25 (4): 667–78.\n\n\nWirbel, Jakob, Paul Theodor Pyl, Ece Kartal, Konrad Zych, Alireza Kashani, Alessio Milanese, Jonas S Fleck, et al. 2019. “Meta-Analysis of Fecal Metagenomes Reveals Global Microbial Signatures That Are Specific for Colorectal Cancer.” Nature Medicine 25 (4): 679–89.",
    "crumbs": [
      "Colorectal Cancer Detection"
    ]
  },
  {
    "objectID": "raw_to_data.html",
    "href": "raw_to_data.html",
    "title": "Processing metagenomic raw sequences",
    "section": "",
    "text": "Metagenomic raw sequences consist of strings composed of four letters: A, C, T, and G, representing the nucleotide bases Adenine, Cytosine, Thymine, and Guanine, respectively. These bases form the building blocks of DNA. Machines that sequence DNA extract these bases from a DNA sample and store them in files, commonly in formats such as FASTQ or FASTA.\nIn this tutorial, we will focus on the widely adopted FASTQ format. A FASTQ file typically contains essential information about the sample, the nucleotide sequences, and a quality score for each identified base. The quality score indicates the probability of correctly identifying a specific base, providing a valuable measure of sequencing accuracy.\nWe will use FASTQ files to prepare data for further analysis. The figure below illustrates the processing pipeline and the tools involved in this workflow.\n\n\n\n\n\nflowchart LR\n    A(FASTQ) --&gt; B(Quality Control)\n    B --&gt; I(Host DNA Removal)\n    I --&gt; C(Sequence Assembly)\n    C --&gt; D(Taxonomic Profiling)\n    C --&gt; E(Functional Annotation)\n    D --&gt; F(Taxonomic Profile)\n    E --&gt; G(Functional Pathways)\n\n\n\n\n\n\n\n\nQuality Control: This is the first step where we will assess the quality of raw sequences to identify low-quality reads and adapter contamination. Tools such as FASTQC, MULTIQC, and Fastp will help generate quality metrics like per-based quality scores and read length distribution. Trimming tools like Trimmomatic or Trim-glore then help clean the data.\nHuman DNA Removal: This step removes host DNA from metagenomic raw sequences to ensure that the downstream analysis focuses fully to microbial sequences. Tools such as bowtie2 and samtools will be used for this task. The reference human genome (e.g., GRCh38) will be downloaded and indexed before alignment.\nSequence Assembly: In this step, we will assemble short reads into longer sequences. Tools like SPAdes (ideal for small database) and MegaHit (optimized for large datasets) will be employed for assembly.\nTaxonomic Profiling: This step assigns operational taxonomic units (OTUs) to assembeled and quality controled raw sequences to determine the taxonomic composition of the microbiome. To execute this task, we will explore the usage of tools such as MetaPhlan2, BowTie2, and mOTUs.\nFunctional Annotation: While taxonomic profile identifies “who is there”, functional annotation determines “what they are doing”. It assigns functional roles and metabolic pathways present in provide metagenomic sequences. Tools like HUMAnN will be used for the task.",
    "crumbs": [
      "Processing metagenomic raw sequences"
    ]
  },
  {
    "objectID": "raw_to_data_1.html",
    "href": "raw_to_data_1.html",
    "title": "1  Quality control",
    "section": "",
    "text": "1.1 Trimming sequences to enhance quality\nThis task involves removing sequences due to low quality or due to other measures (e.g., shorter than a particular length).\nWe will use here trim-galore (Krueger 2015) to perform the trimming task. The program runs for each paired-eng sequence. Therefore, we will write a script to iterate over all the fastq files in the directory.\nThe following script provides that functionality.",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "raw_to_data_1.html#trimming-sequences-to-enhance-quality",
    "href": "raw_to_data_1.html#trimming-sequences-to-enhance-quality",
    "title": "1  Quality control",
    "section": "",
    "text": "#!/bin/bash\n\n# Create output directory if it doesn't exist\nmkdir -p glore_output\n\n# Loop over all R1 files ending with _1.fastq.gz\nfor r1 in raw_data/*_1.fastq.gz; do\n\n  # Derive the corresponding R2 file by replacing _1 with _2\n  r2=\"raw_data/$(basename \"$r1\" _1.fastq.gz)_2.fastq.gz\"\n\n  # Run Trim Galore for each pair and store output in glore_output\n  trim_galore --nextera  --quality 20 --length 75 --paired \"$r1\" \"$r2\" --output_dir glore_output\ndone",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "remove_human_genome.html",
    "href": "remove_human_genome.html",
    "title": "2  Host DNA Removal",
    "section": "",
    "text": "2.1 Download human genome index files for bowtie\nThe first step is to download bowtie2 indices for human genomes. There are multiple human genomes indices are available here. For this tutorial, we will use hg19 human genome (Thomas et al. 2019).\nThe command below download the genome zipped file and extract human genome indices.",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Host DNA Removal</span>"
    ]
  },
  {
    "objectID": "remove_human_genome.html#download-human-genome-index-files-for-bowtie",
    "href": "remove_human_genome.html#download-human-genome-index-files-for-bowtie",
    "title": "2  Host DNA Removal",
    "section": "",
    "text": "wget https://genome-idx.s3.amazonaws.com/bt/hg19.zip\nunzip hg19.zip",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Host DNA Removal</span>"
    ]
  },
  {
    "objectID": "remove_human_genome.html#remove-human-genome",
    "href": "remove_human_genome.html#remove-human-genome",
    "title": "2  Host DNA Removal",
    "section": "2.2 Remove human genome",
    "text": "2.2 Remove human genome\nThis step consisting of three tasks:\n\nAligning raw sequences to human genomes,\nRemoving mapped sequences\nConverting resultant files back to fastq format.\n\nThe first task is executed by the following command\nbowtie2 -x hg19/hg19  -1 reads_1.fastq.gz -2 reads_2.fastq.gz -S mapped.sam --very-sensitive\nThe second task, which remove mapped sequences, is performed by the following command.\n\n# Convert sam file into bam file\nsamtools view -Sb mapped.sam &gt; mapped.bam\n\n# Extract unmapped sequences\nsamtools view -b -f 12 -F 256 mapped.bam &gt; unmapped.bam\nThe third task is executed by the following command\n# Convert bam file back to fastq format\nsamtools fastq unmapped.bam -1 unmapped_1.fastq -2 unmapped_2.fastq -0 unmapped_singletons.fastq",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Host DNA Removal</span>"
    ]
  },
  {
    "objectID": "chapter5.html#exploring-barteroidetes-firmicutes-and-proteobacteria-for-theis-association-with-crc",
    "href": "chapter5.html#exploring-barteroidetes-firmicutes-and-proteobacteria-for-theis-association-with-crc",
    "title": "10  Statistical analysis",
    "section": "10.3 Exploring Barteroidetes, Firmicutes, and Proteobacteria for theis association with CRc",
    "text": "10.3 Exploring Barteroidetes, Firmicutes, and Proteobacteria for theis association with CRc\nWe go further checking whether these differences are statistically significant or not. We employ Mann-Whitney test which is a non-parametric test for checking significance of differences in values from two independent groups.\nFigure 10.5 below shows distributions of abundance at phylum levels across benign and malign cases for all three selected phylumns. The differences were found to be statistically significant.\n\n\nShow the code\ncolor_palette = {'benign':'green','malign':'red'}\n\n# selected phylums for statistical analysis\nselected_phylums = ['Firmicutes','Proteobacteria','Bacteroidetes']\n\n# extracting only selected phylum data\ndf_selected = df.loc[df['variable'].isin(selected_phylums),:]\n\n# pairs for statistical test\npairs = [\n    (('Firmicutes','benign'), ('Firmicutes','malign')),\n    (('Proteobacteria','benign'), ('Proteobacteria','malign')),\n    (('Bacteroidetes','benign'), ('Bacteroidetes','malign'))\n]\n\n# creating a new figure\nplt.figure()\n\n# plotting boxplot\nax = sns.boxplot(data=df_selected, x='variable', y='value',hue='study_condition', palette=color_palette)\n\n# adding statistical annotation from Mann-Whitney test\nax, test_results = add_stat_annotation(ax, box_pairs=pairs, data=df_selected, x='variable', y='value', \n                                       hue='study_condition',hue_order=['benign','malign'], \n                                       \n                                       test='Mann-Whitney', text_format='star',comparisons_correction=None, \n                                       loc='inside', verbose=False)\n\nplt.xlabel('Phylums')\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10.5: Phylum differences among benign and malign cases\n\n\n\n\n\nLets go further and check which genus and species under these phylums are statistically different in terms of relative abundance among benign and malign tumors.\n\n\nShow the code\ntaxa_selected = taxa_table.loc[taxa_table['phylum'].isin(selected_phylums),:]\n\n# fetching species related to selected phylums\nselected_species_cols = list(taxa_table['species'].unique())\n\n# fetching genus related to selected phylums\nselected_genus_cols = list(taxa_table['genus'].unique())\n\n# fetching family related to selected phylums\nselected_family_cols = list(taxa_table['family'].unique())\n\n# fetching order related to selected phylums\nselected_order_cols = list(taxa_table['order'].unique())\n\n# fetching order related to selected phylums\nselected_class_cols = list(taxa_table['class'].unique())\n\n\n\n\nShow the code\nfrom statannotations.Annotator import Annotator\n\ndef plot_selected_taxa(selected_taxa, plot_at,figsize=(7,15),log_scale=False,title=\"\"):\n    \"\"\"\n    Args:\n    ----\n        selected_taxa(str): taxa which are selected for further exploration\n        taxa_abun_df (dataframe): relative abundance data at taxa\n        plot_at (str): taxa at which distribution will be plotted for benign and malign tumors\n    \n    \n    \"\"\"\n    df_abundance = aggregate_by_taxonomy(otu_table, taxa_table, plot_at)\n    metadata_ = metadata.set_index(metadata['subjectID'])\n\n    taxa_abundance_selected = df_abundance[selected_taxa]\n\n    taxa_abundance_selected['study_condition'] = metadata_['study_condition'].apply(\n    lambda x: 'malign' if x == 'CRC' else 'benign')\n\n    \n    pairs = []\n    \n    for col in taxa_abundance_selected.columns:\n        if col != 'study_condition':\n            pairs.append(((col,'benign'),(col,'malign')))\n\n    plt.figure(figsize=figsize)\n    plot_df = taxa_abundance_selected.melt(id_vars='study_condition',value_vars=selected_taxa)\n    \n    ax = sns.boxplot(data=plot_df,x='variable',y='value',hue='study_condition', palette=color_palette)\n    add_stat_annotation(ax, box_pairs=pairs, data=plot_df, x='variable', y='value', \n                                       hue='study_condition',hue_order=['benign','malign'], \n                                       \n                                       test='Mann-Whitney', text_format='star',comparisons_correction=None, \n                                       loc='inside', verbose=False)  \n    ax.set_ylabel(plot_at)\n    plt.xticks(rotation='vertical')\n    if log_scale:\n        ax.set_yscale('log')\n    plt.title(title)\n    plt.show()\n\n\n\n\nShow the code\nplot_selected_taxa(selected_family, plot_at='family', figsize=(50,5),title='families from selected phylums')\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_selected_taxa(selected_genus, plot_at='genus', figsize=(80,5),title='genus from selected phylums')",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Statistical analysis</span>"
    ]
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "11  Building a CRC classifier",
    "section": "",
    "text": "11.1 Target class distribution across cohorts\nFigure 11.2 below shows the ditribution of target class. For our modeling task, we have grouped adenoma and control into a single class of benign tumor. While, CRC class is treated as malign tumor.",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#association-between-hosts-characteristics-and-crc",
    "href": "chapter6.html#association-between-hosts-characteristics-and-crc",
    "title": "8  Building a CRC classifier",
    "section": "",
    "text": "8.1.1 Age, BMI, and CRC\nWe start by investigating statistical significance of differences between age of control and CRC casese. For this test, we use Mann-Whitney test to investigate statistical significance of differences.\nThe below figures show that **there are statistical significant differences between control and CRC groups.\n\n\n\n\n\n\nAge\n\n\n\nWe found statistical differences in age of control and CRC group.\n\n\n\n\nShow the code\nfrom skbio.diversity import alpha\nfrom statannot import add_stat_annotation\n\n# functions to process the data and prepare in a format supporting phyloseq analysis\ndef get_sample_table(df):\n    df = df[metadata_colnames]\n    df = df.set_index('subjectID')\n    df.drop(['dataset_name','sampleID'],axis=1,inplace=True)\n    return df\n\n\ndef get_otu_taxa_table(df):\n    \"\"\"\n    This function returns otu table that contains relative abundance of species \n    where columns are species and rows are cases.\n    \n    Args:\n        df (dataframe): Dataframe of realtive abundance and metadata\n    \"\"\"\n    df = df[bacteria_colnames + ['subjectID']]\n    df.columns = [\"OTU_{}\".format(str(ind)) for ind, col in enumerate(bacteria_colnames)] + ['subjectID']\n    df.index = df['subjectID']\n    taxa_table = get_taxa_table(bacteria_colnames)\n    return df, taxa_table\n\n\ndef get_taxa_table(list_of_otus):\n    \"\"\"\n    This function parse all present microbial species at different heirarchy levels, e.g., class, order, phylum.\n    \n    \"\"\"\n    otu = 0\n    mapping = {}\n    taxa_cols = ['kingdom','phylum','class','order','family','genus','species']\n    df = pd.DataFrame(columns=taxa_cols)\n    otu_mapping = {}\n    otu_ids = []\n    for ind, otu in enumerate(list_of_otus):\n        tmp = {}\n        for col in taxa_cols:\n            tmp[col] = get_specific_label(otu, col)\n        tmp_df = pd.DataFrame([tmp])\n        df = pd.concat([df,tmp_df], ignore_index=[0])\n\n        otu_id = \"OTU_{}\".format(str(ind))\n        otu_mapping[otu] = otu_id\n        otu_ids.append(otu)\n\n    df['OTU'] = ['OTU_{}'.format(str(ind)) for ind in df.index]\n    df = df.set_index('OTU')\n    return df\n\n\ndef get_specific_label(l, t):\n    \"\"\"\n    This function parse the taxonomic assignment lable and fetch the specified information (e.g., kingdom, family)\n\n    Args:\n        l (str): string of taxonomy\n        t (str): string specifying the requested information (e.g., kingdom, family, genus, etc.)\n\n    Returns:\n        str: requested heirarichal info \n    \"\"\"\n    taxa_order = {'kingdom':0,'phylum':1,'class':2,'order':3,'family':4,'genus':5,'species':6}\n\n    try:\n        specific_label = l.split('|')[taxa_order[t]]\n\n        return specific_label.strip().split('__')[1]\n    except:\n        return 'Unknown'\n\n    \ndef get_otu_detail(taxa_table, otu_label, rank):\n    return taxa_table[otu_label][rank]\n\n\ndef aggregate_by_taxonomy(otu_table, taxa_table, taxa_rank):\n    \"\"\"\n    This function aggregates data based on specified \n    taxa rank (e.g., kingdom, class, order, phylum, genus, species).\n    \"\"\"\n    unique_values = (taxa_table[taxa_rank].unique())\n\n    # mapping for otus to unique value of chosen taxa rank\n    taxa_to_otu = {}\n\n    # prepare the mapping\n    for unique_value in unique_values:\n        tdf = taxa_table.loc[taxa_table[taxa_rank] == unique_value, :]\n        otus = tdf.index.to_list()   \n        taxa_to_otu[unique_value.strip()] = otus\n    \n    # create a dictionary for formulating expressions\n    taxa_to_exp = {}\n\n    for key in taxa_to_otu.keys():\n        taxa_to_exp[key] = '{} = 0.000001'.format(key)\n        for otu in taxa_to_otu[key]:\n            taxa_to_exp[key] += ' + ' + otu\n            otu_table[otu] = pd.to_numeric(otu_table[otu], errors='coerce')\n\n    agg_df = otu_table\n    \n    for key, expr in taxa_to_exp.items():\n        agg_df[key] = 0\n        agg_df = agg_df.eval(expr, engine='python')\n        \n    agg_df = agg_df[list(unique_values)]\n    \n    return agg_df   \n\n\ndef extend_with_alpha(df, metadata_features):\n    \"\"\"\n    This function extends the dataframe with alpha diversity measures.\n    \n    Args:\n        df: dataframe\n        \n        metadata_features: list of metadata feature names\n        \n    Returns:\n        dataframe: extended dataframe with alpha diversity features\n    \"\"\"\n    diversity_measures = pd.DataFrame()\n\n    alpha_diversity_metrics = [\n        \"chao1\",\n        \"shannon\",\n        \"simpson\",\n        \"simpson_e\",\n        \"fisher_alpha\",\n        \"berger_parker\"\n    ]\n\n    shannon_diversity = df.apply(lambda x: alpha.shannon(x), axis=1)\n    chao1_diversity   = df.apply(lambda x: alpha.chao1(x), axis=1)\n    simpson_diversity   = df.apply(lambda x: alpha.simpson(x), axis=1)\n    simpson_e_diversity   = df.apply(lambda x: alpha.simpson_e(x), axis=1)\n    fisher_diversity   = df.apply(lambda x: alpha.fisher_alpha(x), axis=1)\n    berger_parker_diversity   = df.apply(lambda x: alpha.berger_parker_d(x), axis=1)\n\n    diversity_measures['shannon'] = shannon_diversity\n    diversity_measures['chao1'] = chao1_diversity\n    diversity_measures['simpson'] = simpson_diversity\n    diversity_measures['simpson_e'] = simpson_e_diversity\n    diversity_measures['fisher_alpha'] = fisher_diversity\n    diversity_measures['berger_parker'] = berger_parker_diversity\n    \n    X_alpha = diversity_measures.reset_index().drop(['subjectID'], axis=1)\n    X_extended = pd.concat([metadata_features,X_alpha],axis=1)\n    \n    return X_extended\n\n# convert data tables into otu and taxa table\notu_table, taxa_table = get_otu_taxa_table(zeller_db)\n\n# aggregating data at higher levels\nphylum_agg = aggregate_by_taxonomy(otu_table, taxa_table, 'phylum')\ngenus_agg = aggregate_by_taxonomy(otu_table, taxa_table, 'genus')\norder_agg = aggregate_by_taxonomy(otu_table, taxa_table, 'order')\n\ncolor_palette = {'control':'green',\n                'adenoma':'orange',\n                'CRC':'#c80000'}\n\norder = ['control', 'adenoma', 'CRC']\nx = 'study_condition'\n\npairs = [\n    ('control','adenoma'),\n    ('control','CRC'),\n    ('adenoma','CRC'),\n]\n\nmetadata = zeller_db[metadata_colnames]\n\n# changing data type of age and BMI\nmetadata['age'] = pd.to_numeric(metadata.age, errors='coerce')\nmetadata['BMI'] = pd.to_numeric(metadata.BMI, errors='coerce')\n\n\n\nShow the code\n# plotting distribution\n\nfor ind, y in enumerate(['age','BMI']):\n    plt.figure()\n    ax = sns.boxplot(data=metadata, y=y,x=x, palette=color_palette, order= order)\n    #annot = Annotator(ax, pairs=pairs, data=metadata, x=x, y=y, hue=x, hue_order=order, order=order)\n    ax, test_results = add_stat_annotation(ax, box_pairs=pairs, data=metadata, x=x, y=y,\n                                           hue_order=order, order=order,\n                                           test='Mann-Whitney', text_format='star',comparisons_correction=None, \n                                           loc='inside', verbose=False)\n\n    plt.title(f'{y.upper()}')\n\n    plt.show()\n    \n\n\n\n\n\n\n\n\n\n\n\n\n(a) Age\n\n\n\n\n\n\n\n\n\n\n\n(b) BMI\n\n\n\n\n\n\n\nFigure 8.1: Age/BMI distribution\n\n\n\n\n\n8.1.2 Gender and CRC\nWe now check the distribution of gender across different target groups (i.e., control, adenoma, CRC). We employ the Chi-squared test to investigate the statistical significance of differences in gender distribution across different groups.\nFigure 8.2 shows the frequency count of males/females across control, adenoma, and CRC groups. The differences were found to be statistically significant (p-value &lt; .05).\n\n\n\n\n\n\nGender\n\n\n\nWe also found statistical differences in gender between control, adenoma and CRC group.\n\n\n\n\nShow the code\nfrom scipy.stats import chi2_contingency\n\n# Create a contingency table\ncontingency_table = pd.crosstab(metadata['gender'], metadata['study_condition'])\n\n# Apply Fisher's Exact Test\nchi2, p_value, dof, expected = chi2_contingency(contingency_table)\n\n# new figure\nplt.figure()\n\n# plot frequency plot\nsns.countplot(data=metadata, x='study_condition',hue='gender')\n\n# add p-value\nplt.text(0.5, 32, f'$X^2 test$ p-value: {p_value:.4f}', fontsize=12, color='blue')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8.2: Gender distribution",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#association-between-hosts-characteristics-and-species-abundance",
    "href": "chapter6.html#association-between-hosts-characteristics-and-species-abundance",
    "title": "8  Building a CRC classifier",
    "section": "8.1 Association between host’s characteristics and species abundance",
    "text": "8.1 Association between host’s characteristics and species abundance\nWe will now explore for association between host’s characteristics (e.g., age, BMI) and species relative abundace. Figure 8.3 below shows correlation between host’s characteristics and phylum level abundance data.\n\nFirnicutes, Spirochaetes, and Verrucomicrobia phylum levels are negatively correlated with age. That means as a person gets older these three phylum levels tend to get decreased.\nIn case of BMI, Spirochaetes, Firnicutes, Deferribacteres, Bacteroidetes, and Actinobacteria are found negatively correlated. That implies an increase in BMI (which could be taken as an indication of obesity) is associated with decrease in those phylums.\n\n\nShow the code\n# plotting distribution\n\ndef extend_abundance_metadata(df,meta):\n    \"\"\"\n    This function extends abundance data with metadata information.\n    \n    Args:\n        df (DataFrame): relative abundance data\n        meta (DataFrame): host's characteristics\n        \n    Returns:\n        DataFrame\n    \"\"\"\n    return pd.concat([df,meta],axis=1)\n\n# relative abundance aggregation at \n#family_abundance = aggregate_by_taxonomy(otu_table, taxa_table, 'family')\ngenus_abundance = aggregate_by_taxonomy(otu_table, taxa_table, 'genus')\nphylum_abundance = aggregate_by_taxonomy(otu_table, taxa_table, 'phylum')\nmetadata_ = metadata.set_index(metadata['subjectID'])\n\n# plot age correlation plot\nphylum_metadata = extend_abundance_metadata(phylum_abundance,metadata_[['age']])\nphylum_corr = phylum_metadata.corr()\nplt.figure(figsize=(7,10))\ndata_plot = phylum_corr['age'].drop('age')\nbars = plt.barh(data_plot.index, data_plot, color=np.where(data_plot &gt; 0, 'green', 'red'))\nplt.xlim([-.5,.5])\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.title('Age')\nplt.show()\n\n# plot bmi correlation plot\nphylum_metadata = extend_abundance_metadata(phylum_abundance,metadata_[['BMI']])\nphylum_corr = phylum_metadata.corr()\nplt.figure(figsize=(7,10))\ndata_plot = phylum_corr['BMI'].drop('BMI')\nbars = plt.barh(data_plot.index, data_plot, color=np.where(data_plot &gt; 0, 'green', 'red'))\nplt.xlim([-.5,.5])\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.title('BMI')\nplt.show()\n    \nphylum_metadata_ = phylum_metadata.copy()\nphylum_metadata_['study_condition'] = metadata_['study_condition'].apply(\n    lambda x: 'malign' if x == 'CRC' else 'benign')\n\ndf = phylum_metadata_.melt(id_vars='study_condition',value_vars=phylum_agg.columns)\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Correlation between age and phylum level abundance\n\n\n\n\n\n\n\n\n\n\n\n(b) Correlation between BMI and phylum level abundance\n\n\n\n\n\n\n\nFigure 8.3: Correlation with phylum abundace\n\n\n\nFigure 8.4 (c) below shows differences in microbial composition in terms of phylum level abundance among benign and malign tumors. We can notice three phylums differ among benign and malign tumor groups. Those phylums are Proteobacteria, Firnicutes, and Bacteroidetes.\n\nWe combined control with adenoma to create benign tumor class, and CRC class renamed as malign tumor.\n\n\n\nShow the code\nplt.figure(figsize=(7,10))\nsns.boxplot(data=df, y='variable',x='value', order=list(data_plot.index)[::-1], hue='study_condition', palette={'benign':'green','malign':'red'})\nplt.yticks(fontsize=20)\nplt.xticks(fontsize=20)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8.4: CRC and phylum abundace",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#exploring-barteroidetes-firmicutes-and-proteobacteria-for-theis-association-with-crc",
    "href": "chapter6.html#exploring-barteroidetes-firmicutes-and-proteobacteria-for-theis-association-with-crc",
    "title": "8  Building a CRC classifier",
    "section": "8.2 Exploring Barteroidetes, Firmicutes, and Proteobacteria for theis association with CRc",
    "text": "8.2 Exploring Barteroidetes, Firmicutes, and Proteobacteria for theis association with CRc\nWe go further checking whether these differences are statistically significant or not. We employ Mann-Whitney test which is a non-parametric test for checking significance of differences in values from two independent groups.\nFigure 8.5 below shows distributions of abundance at phylum levels across benign and malign cases for all three selected phylumns. The differences were found to be statistically significant.\n\n\nShow the code\ncolor_palette = {'benign':'green','malign':'red'}\n\n# selected phylums for statistical analysis\nselected_phylums = ['Firmicutes','Proteobacteria','Bacteroidetes']\n\n# extracting only selected phylum data\ndf_selected = df.loc[df['variable'].isin(selected_phylums),:]\n\n# pairs for statistical test\npairs = [\n    (('Firmicutes','benign'), ('Firmicutes','malign')),\n    (('Proteobacteria','benign'), ('Proteobacteria','malign')),\n    (('Bacteroidetes','benign'), ('Bacteroidetes','malign'))\n]\n\n# creating a new figure\nplt.figure()\n\n# plotting boxplot\nax = sns.boxplot(data=df_selected, x='variable', y='value',hue='study_condition', palette=color_palette)\n\n# adding statistical annotation from Mann-Whitney test\nax, test_results = add_stat_annotation(ax, box_pairs=pairs, data=df_selected, x='variable', y='value', \n                                       hue='study_condition',hue_order=['benign','malign'], \n                                       \n                                       test='Mann-Whitney', text_format='star',comparisons_correction=None, \n                                       loc='inside', verbose=False)\n\nplt.xlabel('Phylums')\n\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8.5: Phylum differences among benign and malign cases\n\n\n\n\n\nLets go further and check which genus and species under these phylums are statistically different in terms of relative abundance among benign and malign tumors.\n\n\nShow the code\ntaxa_selected = taxa_table.loc[taxa_table['phylum'].isin(selected_phylums),:]\n\n# fetching species related to selected phylums\nselected_species_cols = list(taxa_table['species'].unique())\n\n# fetching genus related to selected phylums\nselected_genus_cols = list(taxa_table['genus'].unique())\n\n# fetching family related to selected phylums\nselected_family_cols = list(taxa_table['family'].unique())\n\n# fetching order related to selected phylums\nselected_order_cols = list(taxa_table['order'].unique())\n\n# fetching order related to selected phylums\nselected_class_cols = list(taxa_table['class'].unique())\n\n\n\n\nShow the code\nfrom statannotations.Annotator import Annotator\n\ndef plot_selected_taxa(selected_taxa, plot_at,figsize=(7,15),log_scale=False,title=\"\"):\n    \"\"\"\n    Args:\n    ----\n        selected_taxa(str): taxa which are selected for further exploration\n        taxa_abun_df (dataframe): relative abundance data at taxa\n        plot_at (str): taxa at which distribution will be plotted for benign and malign tumors\n    \n    \n    \"\"\"\n    df_abundance = aggregate_by_taxonomy(otu_table, taxa_table, plot_at)\n    metadata_ = metadata.set_index(metadata['subjectID'])\n\n    taxa_abundance_selected = df_abundance[selected_taxa]\n\n    taxa_abundance_selected['study_condition'] = metadata_['study_condition'].apply(\n    lambda x: 'malign' if x == 'CRC' else 'benign')\n\n    \n    pairs = []\n    \n    for col in taxa_abundance_selected.columns:\n        if col != 'study_condition':\n            pairs.append(((col,'benign'),(col,'malign')))\n\n    plt.figure(figsize=figsize)\n    plot_df = taxa_abundance_selected.melt(id_vars='study_condition',value_vars=selected_taxa)\n    \n    ax = sns.boxplot(data=plot_df,x='variable',y='value',hue='study_condition', palette=color_palette)\n    add_stat_annotation(ax, box_pairs=pairs, data=plot_df, x='variable', y='value', \n                                       hue='study_condition',hue_order=['benign','malign'], \n                                       \n                                       test='Mann-Whitney', text_format='star',comparisons_correction=None, \n                                       loc='inside', verbose=False)  \n    ax.set_ylabel(plot_at)\n    plt.xticks(rotation='vertical')\n    if log_scale:\n        ax.set_yscale('log')\n    plt.title(title)\n    plt.show()\n\n\n\n\nShow the code\nplot_selected_taxa(selected_family, plot_at='family', figsize=(50,5),title='families from selected phylums')\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_selected_taxa(selected_genus, plot_at='genus', figsize=(80,5),title='genus from selected phylums')",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#relative-abundance-filtering",
    "href": "chapter6.html#relative-abundance-filtering",
    "title": "8  Building a CRC classifier",
    "section": "8.2 Relative abundance filtering",
    "text": "8.2 Relative abundance filtering\nOur dataset has 699 bacterial species or features for our modeling task. That is a large number of features, especially given our relatively small number of cases",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#feature-selection",
    "href": "chapter6.html#feature-selection",
    "title": "11  Building a CRC classifier",
    "section": "11.2 Feature selection",
    "text": "11.2 Feature selection\nOur dataset has 699 bacterial species or features for our modeling task. That is a large number of features, especially given our relatively small number of cases\nWe will employ different methods for feature selection to proceed with a smaller set of features which will allow an easier interpretation of resultant model and importance of features towards CRC classification.\n\n11.2.1 Relative abundance filtering\nThe first filtering technique simply discard features based on relative abundance data. We will discard any bacteria species whose maximum value do not exceeds .001 in any cohort dataest.",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#pipeline-building",
    "href": "chapter6.html#pipeline-building",
    "title": "11  Building a CRC classifier",
    "section": "11.2 Pipeline building",
    "text": "11.2 Pipeline building\nThe below diagram depicts our model building pipeline.\n\n\n\n\n\ngraph TD\n    A[Input Data] --&gt; B1[Filter Relative Abundance Data]\n    B1 --&gt; C1[Feature Selection]\n    A --&gt; B2[Compute Alpha Diversity]\n    A --&gt; B3[Impute Missing Metadata Features]\n    C1 --&gt; D[Combine Features]\n    B2 --&gt; D\n    B3 --&gt; D\n    D --&gt; E[Train ML Model with Nested CV]\n    E --&gt; F[Leave-One-Group-Out CV]\n    E --&gt; G[5-Fold Stratified CV]\n    F --&gt; H[Model Selection]\n    G --&gt; H[Model Selection]\n\n\n\n\n\n\n\n\nShow the code\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import RFE, SelectKBest, SelectFpr, f_classif\nfrom sklearn.linear_model import LogisticRegression\n\n\nclass DropFeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based of feature names for pipeline\n    \"\"\"\n    def __init__(self, variables):\n        self.variables = variables\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        X_dropped = X.drop(self.variables, axis=1)\n        return X_dropped\n    \n\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based of feature names for pipeline\n    \"\"\"\n    def __init__(self, variables):\n        self.variables = variables\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        X_dropped = X[self.variables]\n        return X_dropped\n    \n    \nclass RelativeAbundanceSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based on relative abundance filtering\n    \"\"\"\n    def __init__(self, threshold):\n        self.threshold = threshold\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        columns_to_select = X.columns[X.max(axis=0) &gt; self.threshold]\n        X_selected = X[columns_to_select]\n        return X_selected\n    \n    \nclass BestFeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Select top k features based on specified strategy\n    \"\"\"\n    def __init__(self, estimator=LogisticRegression(), strategy='SelectKBest', n_features=20):\n        self.strategy = strategy\n        self.estimator = estimator\n        self.n_features = n_features\n        self.strategy_options = {'rfe':RFE(self.estimator, \n                                           n_features_to_select=self.n_features),\n                                 'SelectKBest':SelectKBest(f_classif, k=self.n_features)}\n        self.feature_names_out = None\n        self.feature_selector = self.strategy_options[self.strategy]\n        \n    def fit(self, X, y=None):\n        self.feature_selector.fit(X,y)\n        self.feature_names_out = self.feature_selector.get_feature_names_out()\n        return self\n    \n    def transform(self, X):\n        X_selected = self.feature_selector.transform(X)\n        df = pd.DataFrame(X_selected, columns=self.feature_names_out)\n        \n        return df\n    \n        \nclass AlphaFeatureExtender(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer to extend relative abundace data with alpha diversity measure\n    \"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Create an empty dataframe\n        diversity_measures = pd.DataFrame()\n\n        # alpha diversty measures\n        alpha_diversity_metrics = [\n            \"chao1\",\n            \"shannon\",\n            \"simpson\",\n            \"simpson_e\",\n            \"fisher_alpha\",\n            \"berger_parker\"\n        ]\n\n        # Compute alpha diversity measures\n        shannon_diversity = X.apply(lambda x: alpha.shannon(x), axis=1)\n        chao1_diversity   = X.apply(lambda x: alpha.chao1(x), axis=1)\n        simpson_diversity   = X.apply(lambda x: alpha.simpson(x), axis=1)\n        simpson_e_diversity   = X.apply(lambda x: alpha.simpson_e(x), axis=1)\n        fisher_diversity   = X.apply(lambda x: alpha.fisher_alpha(x), axis=1)\n        berger_parker_diversity   = X.apply(lambda x: alpha.berger_parker_d(x), axis=1)\n\n        # Add alpha measures to the dataframe\n        diversity_measures['shannon'] = shannon_diversity\n        diversity_measures['chao1'] = chao1_diversity\n        diversity_measures['simpson'] = simpson_diversity\n        diversity_measures['simpson_e'] = simpson_e_diversity\n        diversity_measures['fisher_alpha'] = fisher_diversity\n        diversity_measures['berger_parker'] = berger_parker_diversity\n\n        return diversity_measures\n\nclass MetaDataImputer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Imputer for specific metadata (i.e., age, genger, BMI)\n    \"\"\"\n    def __init__(self):\n        self.fill_values = dict()\n\n    \n    def fit(self, X, y=None):\n        X['age'] = pd.to_numeric(X['age'], errors='coerce')\n        X['BMI'] = pd.to_numeric(X['BMI'], errors='coerce')\n        \n        self.fill_values['age'] = X['age'].mean()\n        self.fill_values['BMI'] = X['BMI'].mean()\n        self.fill_values['gender'] = X['gender'].mode().values[0]\n        return self\n    \n    def transform(self, X):\n        X['age'] = pd.to_numeric(X['age'], errors='coerce')\n        X['BMI'] = pd.to_numeric(X['BMI'], errors='coerce')\n        \n        for col in X.columns:\n            X[col].fillna(self.fill_values[col], inplace=True)\n        X['gender'] = X['gender'].map({'female':0, 'male':1})\n        return X\n    \n     \ndef prepare_dataset(X):\n    \"\"\"\n    This function convert column names to species name (if all levels are specified)\n    \"\"\"\n    X_ = X.copy()\n\n    new_cols = []\n\n    for col in X_.columns:\n        if 'k__' in col:\n            new_cols.append(get_specific_label(col.strip(), 'species'))\n            X_[col] = X_[col].astype('float32')\n        else:\n            new_cols.append(col)\n\n    X_.columns = new_cols\n    \n    y= X_['target_class'].map({'benign':0,'malign':1})\n    y.reset_index(drop=True, inplace=True)\n    \n    X_.reset_index(drop=True, inplace=True)\n    \n    return X_, y\n\n\n\n\nShow the code\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, roc_curve, auc\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\ndef build_classifier(pipeline, param_grid, X,y, outer_cv=None, inner_cv=None):\n    \"\"\"\n    This builds CRC classifier using nested cross-validation and print hyperparameters of the model.\n\n    Args:\n    -----\n        model (sklearn model): machine learning model object\n        \n        param_grid (dict): parameters grid to search optimized parameters\n        \n        X (dataframe): pandas dataframe of dataset\n        \n        y (list): target class\n        \n        inner_cv_scoring (int): number of folds for inner cross-validation\n        \n        outer_cv_scoring (int): number of folds for outer cross-validation\n        \n\n    Returns:\n    -----\n        classifier (sklearn trained model)\n    \"\"\"\n    \n    # Outer validation: 10-fold cross-validation\n    if outer_cv is None:\n        outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n    # Inner validation: 5-fold cross-validation for hyperparameter tuning\n    if inner_cv is None:\n        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    \n    aucs = []\n    best_performance = 0\n    best_estimator = None\n\n    # Nested cross-validation\n    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n        X_train, X_test = X.iloc[train_idx,:], X.iloc[test_idx,:]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        # Perform GridSearchCV with 10-fold cross-validation\n        grid_search = GridSearchCV(\n            pipeline, param_grid, scoring='roc_auc_ovr', cv=inner_cv, n_jobs=-1\n        )\n\n        # searching parameters first\n        grid_search.fit(X_train, y_train)\n\n        # Best model and hyperparameter\n        best_model = grid_search.best_estimator_\n        #print(f\"Best Hyperparameter: {grid_search.best_params_}\")\n\n        # Fit the model and predict probabilities\n        best_model.fit(X_train, y_train)\n        y_prob = best_model.predict_proba(X_test)\n\n\n        fpr, tpr, _ = roc_curve(y_test, y_prob[:,1])\n        roc_auc = auc(fpr, tpr)\n        \n        if roc_auc &gt; best_performance:\n            best_estimator = best_model\n            best_performance = roc_auc\n        \n        aucs.append(roc_auc)\n        #print(f\"  performance={roc_auc:.2f}\")\n\n    return best_estimator, best_performance, np.std(aucs)\n\nspecies_colnames = [get_specific_label(item.strip(), 'species') for item in bacteria_colnames]\n# Pipeline for species selection using relative abundace data\nabundance_processing = Pipeline([\n    ('abundance_columns', ColumnSelector(species_colnames)),\n    ('species_selector', BestFeatureSelector())\n])\n\n# Pipeline for computing alpha diversity\nalpha_processing = Pipeline([\n    ('abundance_columns', ColumnSelector(species_colnames)),\n    ('alpha_extended', AlphaFeatureExtender())\n])\n\n# Pipeline for imputing metadata\nmeta_processing = Pipeline([\n    ('meta_columns', ColumnSelector(['age','BMI','gender'])),\n    ('meta_imputer', MetaDataImputer())\n])\n\n# Concatenating results \ncombined_features = FeatureUnion([\n    ('abun_part', abundance_processing),\n    ('alpha_part', alpha_processing),\n    ('meta_part', meta_processing)\n])\n\n\nlasso_param_grid = {\n    \"lasso__C\": [0.001, .01, .1, 1, 10, 100],\n}\n\ndt_param_grid = {\n    \"dt__max_depth\": [3,4,5,6],\n    \"dt__criterion\":['gini','entropy'],\n    \"dt__min_samples_split\":[2,4,6,8,10]\n}\n\nrf_param_grid = {\n    \"rf__n_estimators\":[50,100,150,200],\n    \"rf__max_depth\":[3,4,5,6],\n    \n}\n\nlasso_pipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('lasso', LogisticRegression(penalty='l1',solver='liblinear', max_iter=10000))\n])\n\ndt_pipe = Pipeline([\n    ('dt', DecisionTreeClassifier())\n])\n\nrf_pipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('rf', RandomForestClassifier())\n])\n\n\n\n\nShow the code\n\nclf, per, st = build_classifier(lasso_pipe, lasso_param_grid, train_X, train_y)\n\nprint(f' Best estimator (performance={per:.2f}):')\nprint(clf)\n\n\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\n Best estimator (performance=0.90):\nPipeline(steps=[('scaler', StandardScaler()),\n                ('lasso',\n                 LogisticRegression(C=100, max_iter=1000, penalty='l1',\n                                    solver='liblinear'))])\n\n\n\n\nShow the code\nclf, per, st = build_classifier(dt_pipe, dt_param_grid, train_X, train_y)\n\nprint(f' Best dt estimator (performance={per:.2f}±{st:.2f}):')\nprint(clf)\n\n\n Best dt estimator (performance=0.83±0.09):\nPipeline(steps=[('dt',\n                 DecisionTreeClassifier(criterion='entropy', max_depth=4,\n                                        min_samples_split=10))])\n\n\n\n\nShow the code\nclf, per, st = build_classifier(rf_pipe, rf_param_grid, train_X, train_y)\n\nprint(f' Best rf estimator (performance={per:.2f}±{st:.2f}):')\nprint(clf)\n\n\n Best rf estimator (performance=0.93±0.07):\nPipeline(steps=[('scaler', StandardScaler()),\n                ('rf', RandomForestClassifier(max_depth=6, n_estimators=200))])",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#performance-of-selected-model-using-leaveonegroupout",
    "href": "chapter6.html#performance-of-selected-model-using-leaveonegroupout",
    "title": "8  Building a CRC classifier",
    "section": "8.5 Performance of selected model using LeaveOneGroupOut",
    "text": "8.5 Performance of selected model using LeaveOneGroupOut\nWe will now assess these models using leavel-one-group-out strategy. In this strategy, we will keep a cohort for testing while other cohorts datasets will be used for training. This will give us a better idea about our models’ performance.\n\n\nShow the code\nfrom sklearn.model_selection import LeaveOneGroupOut\n\ndef perform_logo(estimator, X, y, groups,title=''):\n    \"\"\"\n    This function performs Leave-one-group-out evaluation of the model.\n    \"\"\"\n    \n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    aucs = []\n    best_performance = 0\n    best_estimator = None\n    \n    logo = LeaveOneGroupOut()\n    \n    # Nested cross-validation\n    for fold, (train_idx, test_idx) in enumerate(logo.split(X, y,groups)):\n        X_train, X_test = X.iloc[train_idx,:], X.iloc[test_idx,:]\n        y_train, y_test = y[train_idx], y[test_idx]\n    \n        estimator.fit(X_train, y_train)\n        \n        y_prob = estimator.predict_proba(X_test)\n\n        fpr, tpr, _ = roc_curve(y_test, y_prob[:,1])\n        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        roc_auc = auc(fpr, tpr)\n\n        aucs.append(roc_auc)\n        \n    # Average AUC across classes\n    mean_auc = np.mean(aucs)\n\n    # Plot the mean ROC curve\n    # Calculate mean and standard deviation of TPRs\n    mean_tpr = np.mean(tprs, axis=0)\n    std_tpr = np.std(tprs, axis=0)\n    \n    # Plot the mean ROC curve\n    plt.plot(mean_fpr, mean_tpr, color='blue', linestyle='--', lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')\n\n    # Fill the area between the mean TPR and ±1 standard deviation\n    plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, color='blue', alpha=0.2, label='± 1 Std. Dev.')\n\n    # Plot the random chance line\n    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', lw=2, label='Chance')\n\n    # Finalize the plot\n    plt.title(title)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc='lower right')\n    #plt.grid(alpha=0.3)\n    plt.show()\n\n\n\n\nShow the code\ngroups = train_X_df['groups'].map({'france':0,'austria':1,'germany':2,'italy':3})\ngroups = groups.reset_index(drop=True)\n\n\n\nShow the code\ntrain_X, train_y = prepare_dataset(train_X_df)\n\nperform_logo(lg, train_X,train_y, groups,'Logistic regression across cohorts (training)')\nperform_logo(dt, train_X,train_y, groups,'Decision tree across cohorts (training)')\nperform_logo(rf, train_X,train_y, groups,'Random forest across cohorts (training)')\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Logistic regression\n\n\n\n\n\n\n\n\n\n\n\n(b) Decision Tree\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Random Forest\n\n\n\n\n\n\n\nFigure 8.2: Model training performance across cohorts (LOGO)",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#best-models",
    "href": "chapter6.html#best-models",
    "title": "11  Building a CRC classifier",
    "section": "11.4 Best models",
    "text": "11.4 Best models\nWe selected three different machine learning models based on their performance on entire dataset. The models were evluated in a nested cross-validation. The outer cross-validation (10-fold) was used for model performance while the inner cross-validation (5-fold) was used for hyperparameter tuning.\n\nLogisticRegression(C=100) .90 AUC\nDecisionTreeClassifier(criterion=entropy, max_depth=4, min_samples_split=10) AUC= 0.83±0.09\nRandomForestClassifier(max_depth=6, n_estimators=200) AUC = 0.93±0.07\n\n\n\nShow the code\nlg = LogisticRegression(C=100)\ndt = DecisionTreeClassifier(criterion='entropy', max_depth=4, min_samples_split=10) \nrf = RandomForestClassifier(max_depth=6, n_estimators=200)",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#model-performance-of-selected-model-using-leaveonegroupout",
    "href": "chapter6.html#model-performance-of-selected-model-using-leaveonegroupout",
    "title": "11  Building a CRC classifier",
    "section": "11.5 Model performance of selected model using LeaveOneGroupOut",
    "text": "11.5 Model performance of selected model using LeaveOneGroupOut\nWe will now assess these models using leavel-one-group-out strategy. In this strategy, we will keep a cohort for testing while other cohorts datasets will be used for training. This will give us a better idea about our models’ performance.\n\n\nShow the code\nfrom sklearn.model_selection import LeaveOneGroupOut\n\ndef perform_logo(estimator, X, y, groups,title=''):\n    \"\"\"\n    This function performs Leave-one-group-out evaluation of the model.\n    \"\"\"\n    \n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0, 1, 100)\n    aucs = []\n    best_performance = 0\n    best_estimator = None\n    \n    logo = LeaveOneGroupOut()\n    \n    # Nested cross-validation\n    for fold, (train_idx, test_idx) in enumerate(logo.split(X, y,groups)):\n        X_train, X_test = X.iloc[train_idx,:], X.iloc[test_idx,:]\n        y_train, y_test = y[train_idx], y[test_idx]\n    \n        estimator.fit(X_train, y_train)\n        \n        y_prob = estimator.predict_proba(X_test)\n\n        fpr, tpr, _ = roc_curve(y_test, y_prob[:,1])\n        interp_tpr = np.interp(mean_fpr, fpr, tpr)\n        interp_tpr[0] = 0.0\n        tprs.append(interp_tpr)\n        roc_auc = auc(fpr, tpr)\n\n        aucs.append(roc_auc)\n        \n    # Average AUC across classes\n    mean_auc = np.mean(aucs)\n\n    # Plot the mean ROC curve\n    # Calculate mean and standard deviation of TPRs\n    mean_tpr = np.mean(tprs, axis=0)\n    std_tpr = np.std(tprs, axis=0)\n    \n    # Plot the mean ROC curve\n    plt.plot(mean_fpr, mean_tpr, color='blue', linestyle='--', lw=2, label=f'Mean ROC (AUC = {mean_auc:.2f})')\n\n    # Fill the area between the mean TPR and ±1 standard deviation\n    plt.fill_between(mean_fpr, mean_tpr - std_tpr, mean_tpr + std_tpr, color='blue', alpha=0.2, label='± 1 Std. Dev.')\n\n    # Plot the random chance line\n    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', lw=2, label='Chance')\n\n    # Finalize the plot\n    plt.title(title)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.legend(loc='lower right')\n    #plt.grid(alpha=0.3)\n    plt.show()\n\n\n\n\nShow the code\ngroups = train_X_df['groups'].map({'france':0,'austria':1,'germany':2,'italy':3})\ngroups = groups.reset_index(drop=True)\n\n\n\nShow the code\n#train_X, train_y, transformer = prepare_dataset(train_X_df)\n\nperform_logo(lg, train_X,train_y, groups,'Logistic regression across cohorts (training)')\nperform_logo(dt, train_X,train_y, groups,'Decision tree across cohorts (training)')\nperform_logo(rf, train_X,train_y, groups,'Random forest across cohorts (training)')\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Logistic regression\n\n\n\n\n\n\n\n\n\n\n\n(b) Decision Tree\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Random Forest\n\n\n\n\n\n\n\nFigure 11.3: Model training performance across cohorts (LOGO)\n\n\n\n\n11.5.1 Model performance on test data from different cohorts\nNow, we will evaluate our trained model which were trained using data from different cohorts. This evaluation is done on a seperate dataset which was kept aside earlier.\n\n\nShow the code\nlg.fit(train_X, train_y)\ndt.fit(train_X, train_y)\nrf.fit(train_X, train_y)\n\n\nRandomForestClassifier(max_depth=6, n_estimators=200)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  RandomForestClassifier?Documentation for RandomForestClassifieriFittedRandomForestClassifier(max_depth=6, n_estimators=200) \n\n\n\n\nShow the code\ntrain_pre_X, train_y = prepare_dataset(train_X_df)\n\ncombined_features.fit(train_pre_X, train_y)\n\ntrain_X = combined_features.transform(train_pre_X)\n\n\n\n\nShow the code\nfrom sklearn.metrics import RocCurveDisplay\n\nestimator = lg\n\nmean_fpr = np.linspace(0, 1, 100)\ncn_test_set = test_X_df.loc[test_X_df['country']=='ITA',:]\ncn_test_pre_X, cn_test_y = prepare_dataset(cn_test_set)\ncn_test_X = combined_features.transform(cn_test_pre_X)\ny_prob = estimator.predict_proba(cn_test_X)\nfpr, tpr, _ = roc_curve(cn_test_y, y_prob[:,1])\nroc_auc = auc(fpr, tpr)\ninterp_tpr = np.interp(mean_fpr, fpr, tpr)\ninterp_tpr[0] = 0.0\n\n# Plot the mean ROC curve\nplt.plot(mean_fpr, interp_tpr, color='blue', linestyle='--', lw=2, label=f'French (AUC = {roc_auc:.2f})')\nplt.legend()",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "seq_assembly.html",
    "href": "seq_assembly.html",
    "title": "3  Sequence Assembly",
    "section": "",
    "text": "References",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequence Assembly</span>"
    ]
  },
  {
    "objectID": "taxo_profile.html",
    "href": "taxo_profile.html",
    "title": "4  Taxonomic Profiling",
    "section": "",
    "text": "References",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Taxonomic Profiling</span>"
    ]
  },
  {
    "objectID": "fun_profile.html",
    "href": "fun_profile.html",
    "title": "5  Functional Profiling",
    "section": "",
    "text": "References",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "raw_to_data_1.html#references",
    "href": "raw_to_data_1.html#references",
    "title": "1  Quality control",
    "section": "References",
    "text": "References\n\n\n\n\nEwels, Philip, Måns Magnusson, Sverker Lundin, and Max Käller. 2016. “MultiQC: Summarize Analysis Results for Multiple Tools and Samples in a Single Report.” Bioinformatics 32 (19): 3047–48.\n\n\nKrueger, Felix. 2015. “Trim Galore!: A Wrapper Around Cutadapt and FastQC to Consistently Apply Adapter and Quality Trimming to FastQ Files, with Extra Functionality for RRBS Data.” Babraham Institute.",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Quality control</span>"
    ]
  },
  {
    "objectID": "remove_human_genome.html#references",
    "href": "remove_human_genome.html#references",
    "title": "2  Host DNA Removal",
    "section": "References",
    "text": "References\n\n\n\n\nLangmead, Ben, and Steven L Salzberg. 2012. “Fast Gapped-Read Alignment with Bowtie 2.” Nature Methods 9 (4): 357–59.\n\n\nLi, Heng. 2009. “The Sequence Alignment/Map (SAM) Format and SAMtools 1000 Genome Project Data Processing Subgroup.” Bioinformatics 25: 1.\n\n\nThomas, Andrew Maltez, Paolo Manghi, Francesco Asnicar, Edoardo Pasolli, Federica Armanini, Moreno Zolfo, Francesco Beghini, et al. 2019. “Metagenomic Analysis of Colorectal Cancer Datasets Identifies Cross-Cohort Microbial Diagnostic Signatures and a Link with Choline Degradation.” Nature Medicine 25 (4): 667–78.",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Host DNA Removal</span>"
    ]
  },
  {
    "objectID": "seq_assembly.html#references",
    "href": "seq_assembly.html#references",
    "title": "3  Sequence Assembly",
    "section": "",
    "text": "Li, Dinghua, Chi-Man Liu, Ruibang Luo, Kunihiko Sadakane, and Tak-Wah Lam. 2015. “MEGAHIT: An Ultra-Fast Single-Node Solution for Large and Complex Metagenomics Assembly via Succinct de Bruijn Graph.” Bioinformatics 31 (10): 1674–76.\n\n\nTran, Quang, and Vinhthuy Phan. 2020. “Assembling Reads Improves Taxonomic Classification of Species.” Genes 11 (8): 946.",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Sequence Assembly</span>"
    ]
  },
  {
    "objectID": "taxo_profile.html#references",
    "href": "taxo_profile.html#references",
    "title": "4  Taxonomic Profiling",
    "section": "",
    "text": "Ruscheweyh, Hans-Joachim, Alessio Milanese, Lucas Paoli, Anna Sintsova, Daniel R Mende, Georg Zeller, and Shinichi Sunagawa. 2021. “mOTUs: Profiling Taxonomic Composition, Transcriptional Activity and Strain Populations of Microbial Communities.” Current Protocols 1 (8): e218.",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Taxonomic Profiling</span>"
    ]
  },
  {
    "objectID": "fun_profile.html#references",
    "href": "fun_profile.html#references",
    "title": "5  Functional Profiling",
    "section": "",
    "text": "Franzosa, Eric A, Lauren J McIver, Gholamali Rahnavard, Luke R Thompson, Melanie Schirmer, George Weingart, Karen Schwarzberg Lipson, et al. 2018. “Species-Level Functional Profiling of Metagenomes and Metatranscriptomes.” Nature Methods 15 (11): 962–68.",
    "crumbs": [
      "Processing metagenomic raw sequences",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Functional Profiling</span>"
    ]
  },
  {
    "objectID": "chapter6.html#train-and-test-split-of-dataset",
    "href": "chapter6.html#train-and-test-split-of-dataset",
    "title": "11  Building a CRC classifier",
    "section": "11.2 Train and test split of dataset",
    "text": "11.2 Train and test split of dataset\nWe will split each of aforementioned cohort using 70/30 split rule resulting in a training set of 70% cases and a test set of 30% cases. We will use the test set only for our final evaluation of our models’ performance.\n\n\nShow the code\nfrom sklearn.model_selection import train_test_split\n\n# List to store training and test seperately\ntrain_X_data = []\ntest_X_data = []\n\nfor country in ['france','austria','germany','italy']:\n    # Get country specific dataset\n    dataset = get_country_dataset('Nine_CRC_cohorts_taxon_profiles.tsv',country,country_dataset_mapping)\n\n    y = dataset['target_class']\n    X = dataset.copy()\n    \n    # Split train and test\n    train_x, test_x, train_y, test_y = train_test_split(X,y,test_size=0.20, random_state=42)\n    \n    # Store country\n    train_x['groups'] = country\n    test_x['groups'] = country\n    \n    # Storing train and test seperately\n    train_X_data.append(train_x)\n    test_X_data.append(test_x)\n    \n# Concatenating all country's dataset\ntrain_X_df = pd.concat(train_X_data, axis=0)\ntest_X_df = pd.concat(test_X_data, axis=0)\n\n# Preparing X and y\ntrain_X = train_X_df.drop(['target_class'], axis=1)\ntrain_y = train_X_df['target_class']\n\ntest_X_df.to_csv('test_X_df.csv',index=False)",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#modepipeline-building",
    "href": "chapter6.html#modepipeline-building",
    "title": "11  Building a CRC classifier",
    "section": "11.2 ModePipeline building",
    "text": "11.2 ModePipeline building\nThe below diagram depicts our model building pipeline.\n\n\n\n\n\ngraph TD\n    A[Input Data] --&gt; B1[Filter Relative Abundance Data]\n    B1 --&gt; C1[Feature Selection]\n    A --&gt; B2[Compute Alpha Diversity]\n    A --&gt; B3[Impute Missing Metadata Features]\n    C1 --&gt; D[Combine Features]\n    B2 --&gt; D\n    B3 --&gt; D\n    D --&gt; E[Train ML Model with Nested CV]\n    E --&gt; F[Leave-One-Group-Out CV]\n    E --&gt; G[5-Fold Stratified CV]\n    F --&gt; H[Model Selection]\n    G --&gt; H[Model Selection]\n\n\n\n\n\n\n\n\nShow the code\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import RFE, SelectKBest, SelectFpr, f_classif\nfrom sklearn.linear_model import LogisticRegression\n\n\nclass DropFeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based of feature names for pipeline\n    \"\"\"\n    def __init__(self, variables):\n        self.variables = variables\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        X_dropped = X.drop(self.variables, axis=1)\n        return X_dropped\n    \n\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based of feature names for pipeline\n    \"\"\"\n    def __init__(self, variables):\n        self.variables = variables\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        X_dropped = X[self.variables]\n        return X_dropped\n    \n    \nclass RelativeAbundanceSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based on relative abundance filtering\n    \"\"\"\n    def __init__(self, threshold):\n        self.threshold = threshold\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        columns_to_select = X.columns[X.max(axis=0) &gt; self.threshold]\n        X_selected = X[columns_to_select]\n        return X_selected\n    \n    \nclass BestFeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Select top k features based on specified strategy\n    \"\"\"\n    def __init__(self, estimator=LogisticRegression(), strategy='SelectKBest', n_features=20):\n        self.strategy = strategy\n        self.estimator = estimator\n        self.n_features = n_features\n        self.strategy_options = {'rfe':RFE(self.estimator, \n                                           n_features_to_select=self.n_features),\n                                 'SelectKBest':SelectKBest(f_classif, k=self.n_features)}\n        self.feature_names_out = None\n        self.feature_selector = self.strategy_options[self.strategy]\n        \n    def fit(self, X, y=None):\n        self.feature_selector.fit(X,y)\n        self.feature_names_out = self.feature_selector.get_feature_names_out()\n        return self\n    \n    def transform(self, X):\n        X_selected = self.feature_selector.transform(X)\n        df = pd.DataFrame(X_selected, columns=self.feature_names_out)\n        \n        return df\n    \n        \nclass AlphaFeatureExtender(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer to extend relative abundace data with alpha diversity measure\n    \"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Create an empty dataframe\n        diversity_measures = pd.DataFrame()\n\n        # alpha diversty measures\n        alpha_diversity_metrics = [\n            \"chao1\",\n            \"shannon\",\n            \"simpson\",\n            \"simpson_e\",\n            \"fisher_alpha\",\n            \"berger_parker\"\n        ]\n\n        # Compute alpha diversity measures\n        shannon_diversity = X.apply(lambda x: alpha.shannon(x), axis=1)\n        chao1_diversity   = X.apply(lambda x: alpha.chao1(x), axis=1)\n        simpson_diversity   = X.apply(lambda x: alpha.simpson(x), axis=1)\n        simpson_e_diversity   = X.apply(lambda x: alpha.simpson_e(x), axis=1)\n        fisher_diversity   = X.apply(lambda x: alpha.fisher_alpha(x), axis=1)\n        berger_parker_diversity   = X.apply(lambda x: alpha.berger_parker_d(x), axis=1)\n\n        # Add alpha measures to the dataframe\n        diversity_measures['shannon'] = shannon_diversity\n        diversity_measures['chao1'] = chao1_diversity\n        diversity_measures['simpson'] = simpson_diversity\n        diversity_measures['simpson_e'] = simpson_e_diversity\n        diversity_measures['fisher_alpha'] = fisher_diversity\n        diversity_measures['berger_parker'] = berger_parker_diversity\n\n        return diversity_measures\n\nclass MetaDataImputer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Imputer for specific metadata (i.e., age, genger, BMI)\n    \"\"\"\n    def __init__(self):\n        self.fill_values = dict()\n\n    \n    def fit(self, X, y=None):\n        X['age'] = pd.to_numeric(X['age'], errors='coerce')\n        X['BMI'] = pd.to_numeric(X['BMI'], errors='coerce')\n        \n        self.fill_values['age'] = X['age'].mean()\n        self.fill_values['BMI'] = X['BMI'].mean()\n        self.fill_values['gender'] = X['gender'].mode().values[0]\n        return self\n    \n    def transform(self, X):\n        X['age'] = pd.to_numeric(X['age'], errors='coerce')\n        X['BMI'] = pd.to_numeric(X['BMI'], errors='coerce')\n        \n        for col in X.columns:\n            X[col].fillna(self.fill_values[col], inplace=True)\n        X['gender'] = X['gender'].map({'female':0, 'male':1})\n        return X\n    \n     \ndef prepare_dataset(X):\n    \"\"\"\n    This function convert column names to species name (if all levels are specified)\n    \"\"\"\n    X_ = X.copy()\n\n    new_cols = []\n\n    for col in X_.columns:\n        if 'k__' in col:\n            new_cols.append(get_specific_label(col.strip(), 'species'))\n            X_[col] = X_[col].astype('float32')\n        else:\n            new_cols.append(col)\n\n    X_.columns = new_cols\n    \n    y= X_['target_class'].map({'benign':0,'malign':1})\n    y.reset_index(drop=True, inplace=True)\n    \n    X_.reset_index(drop=True, inplace=True)\n    \n    return X_, y\n\n\n\n\nShow the code\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, roc_curve, auc\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\ndef build_classifier(pipeline, param_grid, X,y, outer_cv=None, inner_cv=None):\n    \"\"\"\n    This builds CRC classifier using nested cross-validation and print hyperparameters of the model.\n\n    Args:\n    -----\n        model (sklearn model): machine learning model object\n        \n        param_grid (dict): parameters grid to search optimized parameters\n        \n        X (dataframe): pandas dataframe of dataset\n        \n        y (list): target class\n        \n        inner_cv_scoring (int): number of folds for inner cross-validation\n        \n        outer_cv_scoring (int): number of folds for outer cross-validation\n        \n\n    Returns:\n    -----\n        classifier (sklearn trained model)\n    \"\"\"\n    \n    # Outer validation: 10-fold cross-validation\n    if outer_cv is None:\n        outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n    # Inner validation: 5-fold cross-validation for hyperparameter tuning\n    if inner_cv is None:\n        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    \n    aucs = []\n    best_performance = 0\n    best_estimator = None\n\n    # Nested cross-validation\n    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n        X_train, X_test = X.iloc[train_idx,:], X.iloc[test_idx,:]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        # Perform GridSearchCV with 10-fold cross-validation\n        grid_search = GridSearchCV(\n            pipeline, param_grid, scoring='roc_auc_ovr', cv=inner_cv, n_jobs=-1\n        )\n\n        # searching parameters first\n        grid_search.fit(X_train, y_train)\n\n        # Best model and hyperparameter\n        best_model = grid_search.best_estimator_\n        #print(f\"Best Hyperparameter: {grid_search.best_params_}\")\n\n        # Fit the model and predict probabilities\n        best_model.fit(X_train, y_train)\n        y_prob = best_model.predict_proba(X_test)\n\n\n        fpr, tpr, _ = roc_curve(y_test, y_prob[:,1])\n        roc_auc = auc(fpr, tpr)\n        \n        if roc_auc &gt; best_performance:\n            best_estimator = best_model\n            best_performance = roc_auc\n        \n        aucs.append(roc_auc)\n        #print(f\"  performance={roc_auc:.2f}\")\n\n    return best_estimator, best_performance, np.std(aucs)\n\nspecies_colnames = [get_specific_label(item.strip(), 'species') for item in bacteria_colnames]\n# Pipeline for species selection using relative abundace data\nabundance_processing = Pipeline([\n    ('abundance_columns', ColumnSelector(species_colnames)),\n    ('species_selector', BestFeatureSelector())\n])\n\n# Pipeline for computing alpha diversity\nalpha_processing = Pipeline([\n    ('abundance_columns', ColumnSelector(species_colnames)),\n    ('alpha_extended', AlphaFeatureExtender())\n])\n\n# Pipeline for imputing metadata\nmeta_processing = Pipeline([\n    ('meta_columns', ColumnSelector(['age','BMI','gender'])),\n    ('meta_imputer', MetaDataImputer())\n])\n\n# Concatenating results \ncombined_features = FeatureUnion([\n    ('abun_part', abundance_processing),\n    ('alpha_part', alpha_processing),\n    ('meta_part', meta_processing)\n])\n\n\nlasso_param_grid = {\n    \"lasso__C\": [0.001, .01, .1, 1, 10, 100],\n}\n\ndt_param_grid = {\n    \"dt__max_depth\": [3,4,5,6],\n    \"dt__criterion\":['gini','entropy'],\n    \"dt__min_samples_split\":[2,4,6,8,10]\n}\n\nrf_param_grid = {\n    \"rf__n_estimators\":[50,100,150,200],\n    \"rf__max_depth\":[3,4,5,6],\n    \n}\n\nlasso_pipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('lasso', LogisticRegression(penalty='l1',solver='liblinear', max_iter=10000))\n])\n\ndt_pipe = Pipeline([\n    ('dt', DecisionTreeClassifier())\n])\n\nrf_pipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('rf', RandomForestClassifier())\n])\n\n\n\n\nShow the code\n\nclf, per, st = build_classifier(lasso_pipe, lasso_param_grid, train_X, train_y)\n\nprint(f' Best estimator (performance={per:.2f}):')\nprint(clf)\n\n\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\n Best estimator (performance=0.90):\nPipeline(steps=[('scaler', StandardScaler()),\n                ('lasso',\n                 LogisticRegression(C=100, max_iter=1000, penalty='l1',\n                                    solver='liblinear'))])\n\n\n\n\nShow the code\nclf, per, st = build_classifier(dt_pipe, dt_param_grid, train_X, train_y)\n\nprint(f' Best dt estimator (performance={per:.2f}±{st:.2f}):')\nprint(clf)\n\n\n Best dt estimator (performance=0.83±0.09):\nPipeline(steps=[('dt',\n                 DecisionTreeClassifier(criterion='entropy', max_depth=4,\n                                        min_samples_split=10))])\n\n\n\n\nShow the code\nclf, per, st = build_classifier(rf_pipe, rf_param_grid, train_X, train_y)\n\nprint(f' Best rf estimator (performance={per:.2f}±{st:.2f}):')\nprint(clf)\n\n\n Best rf estimator (performance=0.93±0.07):\nPipeline(steps=[('scaler', StandardScaler()),\n                ('rf', RandomForestClassifier(max_depth=6, n_estimators=200))])",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#model-building-pipeline",
    "href": "chapter6.html#model-building-pipeline",
    "title": "11  Building a CRC classifier",
    "section": "11.3 Model building pipeline",
    "text": "11.3 Model building pipeline\nThe below diagram depicts our model building pipeline consisting of steps ranging from feature selection, dimensionality reduction to model selection. These steps are explained at length below.\n\n\n\n\n\ngraph TD\n    A(Input Data) --&gt; B1(Relative Abundance Filter)\n    B1 --&gt; I(Variance Threshold Filter)\n    I --&gt; C1(Feature Selection)\n    A --&gt; B2(Compute Alpha Diversity)\n    A --&gt; B3(Impute Missing Metadata Features)\n    A --&gt; B4(PCA)\n    C1 --&gt; D(Combine Features)\n    B2 --&gt; D\n    B3 --&gt; D\n    B4 --&gt; D\n    D --&gt; E(Train ML Model with Nested CV)\n    E --&gt; H(Model Selection)\n\n\n\n\n\n\n\n\n\nShow the code\nX &gt; 0\n\n\nTypeError: '&gt;' not supported between instances of 'str' and 'int'\n\n\n\n\nShow the code\n?resample\n\n\n\n\nShow the code\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.feature_selection import RFECV, SelectKBest, SelectFpr, f_classif, VarianceThreshold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.utils import resample\n\n# Creating custom pipeline processor\nclass ColumnSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based of feature names for pipeline\n    \"\"\"\n    def __init__(self, variables):\n        self.variables = variables\n    \n    def fit(self, X, y = None):\n        return self\n    \n    def transform(self, X):\n        X_dropped = X[self.variables]\n        return X_dropped\n    \n    \nclass RelativeAbundanceFilter(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based on relative abundance filtering\n    \"\"\"\n    def __init__(self, threshold):\n        self.threshold = threshold\n        self.columns_select = None\n        \n    def fit(self, X, y=None):\n        self.columns_to_select = X.columns[X.max(axis=0) &gt; self.threshold]\n        return self\n    \n    def transform(self, X):\n        X_selected = X[self.columns_to_select]\n        return X_selected\n    \n    \nclass VarianceThresholdFilter(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom selector based on relative abundance filtering\n    \"\"\"\n    def __init__(self, threshold=0.0):\n        self.transformer = VarianceThreshold(threshold)\n               \n    def fit(self, X, y=None):\n        self.transformer.fit(X,y)\n        return self\n    \n    def transform(self, X):\n        X_filtered = self.transformer.transform(X)\n        return X_filtered\n    \n    \nclass FeatureSelector(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Select top k features based on specified strategy\n    \"\"\"\n    def __init__(self, cv=10):\n        self.estimator = LassoCV(cv=cv)\n        self.feature_coef = pd.DataFrame()\n        \n    def fit(self, X, y=None):\n        self.feature_coef.columns = X.columns\n        \n        for sample in resample(X,y):\n            self.estimator.fit(X,y, random_state=42)\n            coef_df = pd.DataFrame(self.estimator.coef_, columns=X.columns)\n            self.feature_coef = pd.concat([self.feature_coef, coef_df], axis=0, ignore_index=True)\n            \n        \n        \n        \n        return self\n    \n    def transform(self, X):\n        X_selected = self.feature_selector.transform(X)\n        df = pd.DataFrame(X_selected, columns=self.feature_names_out)\n        \n        return df\n    \n        \nclass AlphaFeatureExtender(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Custom transformer to extend relative abundace data with alpha diversity measure\n    \"\"\"\n    def __init__(self):\n        pass\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        # Create an empty dataframe\n        diversity_measures = pd.DataFrame()\n\n        # alpha diversty measures\n        alpha_diversity_metrics = [\n            \"chao1\",\n            \"shannon\",\n            \"simpson\",\n            \"simpson_e\",\n            \"fisher_alpha\",\n            \"berger_parker\"\n        ]\n\n        # Compute alpha diversity measures\n        shannon_diversity = X.apply(lambda x: alpha.shannon(x), axis=1)\n        chao1_diversity   = X.apply(lambda x: alpha.chao1(x), axis=1)\n        simpson_diversity   = X.apply(lambda x: alpha.simpson(x), axis=1)\n        simpson_e_diversity   = X.apply(lambda x: alpha.simpson_e(x), axis=1)\n        fisher_diversity   = X.apply(lambda x: alpha.fisher_alpha(x), axis=1)\n        berger_parker_diversity   = X.apply(lambda x: alpha.berger_parker_d(x), axis=1)\n\n        # Add alpha measures to the dataframe\n        diversity_measures['shannon'] = shannon_diversity\n        diversity_measures['chao1'] = chao1_diversity\n        diversity_measures['simpson'] = simpson_diversity\n        diversity_measures['simpson_e'] = simpson_e_diversity\n        diversity_measures['fisher_alpha'] = fisher_diversity\n        diversity_measures['berger_parker'] = berger_parker_diversity\n\n        return diversity_measures\n\n    \nclass MetaDataImputer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Imputer for specific metadata (i.e., age, genger, BMI)\n    \"\"\"\n    def __init__(self):\n        self.fill_values = dict()\n\n    \n    def fit(self, X, y=None):\n        X['age'] = pd.to_numeric(X['age'], errors='coerce')\n        X['BMI'] = pd.to_numeric(X['BMI'], errors='coerce')\n        \n        self.fill_values['age'] = X['age'].mean()\n        self.fill_values['BMI'] = X['BMI'].mean()\n        self.fill_values['gender'] = X['gender'].mode().values[0]\n        return self\n    \n    def transform(self, X):\n        X['age'] = pd.to_numeric(X['age'], errors='coerce')\n        X['BMI'] = pd.to_numeric(X['BMI'], errors='coerce')\n        \n        for col in X.columns:\n            X[col].fillna(self.fill_values[col], inplace=True)\n        X['gender'] = X['gender'].map({'female':0, 'male':1})\n        return X\n    \n     \ndef prepare_dataset(X, return_mapping=False):\n    \"\"\"\n    This function prepare dataframe for modeling task\n    \"\"\"\n    \n    otu, taxa = get_otu_table(X)   \n    metadata = get_metadata(X,['age','gender','BMI'])\n\n    for col in otu.columns:\n        otu[col] = otu[col].astype('float32')\n        \n    metadata['age'] = pd.to_numeric(metadata['age'], errors='coerce')\n    metadata['BMI'] = pd.to_numeric(metadata['BMI'], errors='coerce')\n    \n\n    y= X['target_class'].map({'benign':0,'malign':1})\n    y.reset_index(drop=True, inplace=True)    \n    otu.reset_index(drop=True, inplace=True)\n    \n    otu_to_species = taxa['species'].to_dict()\n    \n    X_ = pd.concat([otu, metadata], axis=1)\n    if return_mapping:\n        return X_, y, otu_to_species\n    else:\n        return X_, y\n\n\n\n11.3.1 Applying filters to reduce feature space\nWe will start by reducing feature spaces through the following three steps.\n\nRelative abundance filter\nVariance threshold filter\nBest features selection using Lasso\n\n\n\nShow the code\nX, y, m = prepare_dataset(train_X_df, return_mapping=True)\n\n\n\n\nShow the code\nX_filtered[X_filtered &gt; 0] = 1\nX_filtered[X_filtered &lt;= 0] = 0\n\nX_filtered.sum(axis=0)\n\n\nOTU_0      523.0\nOTU_1      523.0\nOTU_2      523.0\nOTU_3      523.0\nOTU_4      523.0\n           ...  \nOTU_694    523.0\nOTU_695    523.0\nOTU_696    523.0\nOTU_697    523.0\nOTU_698    523.0\nLength: 641, dtype: float32\n\n\n\n\nShow the code\nX_filtered\n\n\n\n\n\n\n\n\n\n\nOTU_0\nOTU_1\nOTU_2\nOTU_3\nOTU_4\nOTU_5\nOTU_6\nOTU_7\nOTU_8\nOTU_9\n...\nOTU_689\nOTU_690\nOTU_691\nOTU_692\nOTU_693\nOTU_694\nOTU_695\nOTU_696\nOTU_697\nOTU_698\n\n\n\n\n0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n1\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n2\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n3\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n4\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n518\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n519\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n520\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n521\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n522\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n...\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\n\n\n523 rows × 641 columns\n\n\n\n\n\n\nShow the code\nrel_filter = RelativeAbundanceFilter(threshold=.001)\nvar_filter = VarianceThresholdFilter()\n\notu_columns = [item for item in X.columns if item not in ['age','gender','BMI']]\n\nfilter_pipe =  Pipeline([\n                    ('abundance',ColumnSelector(otu_columns)),\n                    ('relative',RelativeAbundanceFilter(threshold=.001)),\n                    ('log', LogisticRegression)\n                    ('standard', MinMaxScaler()),\n                    ('variance', VarianceThresholdFilter())\n                    ])\n\nfilter_pipe.fit(X,y)\nX_filtered = filter_pipe.transform(X)\n\n\n\n\nShow the code\nX_filtered.describe()\n\n\n\n\n\n\n\n\n\n\nOTU_0\nOTU_1\nOTU_2\nOTU_3\nOTU_4\nOTU_5\nOTU_6\nOTU_7\nOTU_8\nOTU_9\n...\nOTU_689\nOTU_690\nOTU_691\nOTU_692\nOTU_693\nOTU_694\nOTU_695\nOTU_696\nOTU_697\nOTU_698\n\n\n\n\ncount\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n...\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n523.000000\n\n\nmean\n0.003053\n0.001912\n0.001912\n0.003096\n0.003293\n0.001912\n0.009082\n0.007627\n0.011967\n0.010233\n...\n0.001912\n0.004407\n0.001912\n0.001912\n0.004764\n0.003345\n0.001912\n0.004272\n0.007886\n0.045203\n\n\nstd\n0.047841\n0.043727\n0.043727\n0.048481\n0.045817\n0.043727\n0.061117\n0.062008\n0.076248\n0.063056\n...\n0.043727\n0.061206\n0.043727\n0.043727\n0.059510\n0.054600\n0.043727\n0.049384\n0.077584\n0.115393\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.001023\n\n\n75%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000467\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.028047\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 641 columns\n\n\n\n\n\n\nShow the code\ntest_X, test_y = prepare_dataset(test_X_df)\n\nfilter_pipe.transform(test_X)\n\n\n\n\n\n\n\n\n\n\nOTU_0\nOTU_1\nOTU_2\nOTU_3\nOTU_4\nOTU_5\nOTU_6\nOTU_7\nOTU_8\nOTU_9\n...\nOTU_689\nOTU_690\nOTU_691\nOTU_692\nOTU_693\nOTU_694\nOTU_695\nOTU_696\nOTU_697\nOTU_698\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.000000\n0.0\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.028486\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.000000\n0.0\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.021351\n0.0\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000100\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.008789\n0.0\n0.050711\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000119\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.000000\n0.0\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.025042\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n127\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.000000\n0.0\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.012221\n\n\n128\n0.0\n0.0\n0.0\n0.0\n0.039596\n0.0\n0.000000\n0.0\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.106290\n\n\n129\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.000000\n0.0\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.000000\n\n\n130\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.015126\n0.0\n0.021327\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.022457\n\n\n131\n0.0\n0.0\n0.0\n0.0\n0.000000\n0.0\n0.022920\n0.0\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.002916\n\n\n\n\n132 rows × 641 columns\n\n\n\n\n\n\nShow the code\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, roc_curve, auc\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\ndef build_classifier(pipeline, param_grid, X,y, outer_cv=None, inner_cv=None):\n    \"\"\"\n    This builds CRC classifier using nested cross-validation and print hyperparameters of the model.\n\n    Args:\n    -----\n        model (sklearn model): machine learning model object\n        \n        param_grid (dict): parameters grid to search optimized parameters\n        \n        X (dataframe): pandas dataframe of dataset\n        \n        y (list): target class\n        \n        inner_cv_scoring (int): number of folds for inner cross-validation\n        \n        outer_cv_scoring (int): number of folds for outer cross-validation\n        \n\n    Returns:\n    -----\n        classifier (sklearn trained model)\n    \"\"\"\n    \n    # Outer validation: 10-fold cross-validation\n    if outer_cv is None:\n        outer_cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\n    # Inner validation: 5-fold cross-validation for hyperparameter tuning\n    if inner_cv is None:\n        inner_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n    \n    aucs = []\n    best_performance = 0\n    best_estimator = None\n\n    # Nested cross-validation\n    for fold, (train_idx, test_idx) in enumerate(outer_cv.split(X, y), start=1):\n        X_train, X_test = X.iloc[train_idx,:], X.iloc[test_idx,:]\n        y_train, y_test = y[train_idx], y[test_idx]\n\n        # Perform GridSearchCV with 10-fold cross-validation\n        grid_search = GridSearchCV(\n            pipeline, param_grid, scoring='roc_auc_ovr', cv=inner_cv, n_jobs=-1\n        )\n\n        # searching parameters first\n        grid_search.fit(X_train, y_train)\n\n        # Best model and hyperparameter\n        best_model = grid_search.best_estimator_\n        #print(f\"Best Hyperparameter: {grid_search.best_params_}\")\n\n        # Fit the model and predict probabilities\n        best_model.fit(X_train, y_train)\n        y_prob = best_model.predict_proba(X_test)\n\n\n        fpr, tpr, _ = roc_curve(y_test, y_prob[:,1])\n        roc_auc = auc(fpr, tpr)\n        \n        if roc_auc &gt; best_performance:\n            best_estimator = best_model\n            best_performance = roc_auc\n        \n        aucs.append(roc_auc)\n        #print(f\"  performance={roc_auc:.2f}\")\n\n    return best_estimator, best_performance, np.std(aucs)\n\nspecies_colnames = [get_specific_label(item.strip(), 'species') for item in bacteria_colnames]\n# Pipeline for species selection using relative abundace data\nabundance_processing = Pipeline([\n    ('abundance_columns', ColumnSelector(species_colnames)),\n    ('species_selector', BestFeatureSelector())\n])\n\n# Pipeline for computing alpha diversity\nalpha_processing = Pipeline([\n    ('abundance_columns', ColumnSelector(species_colnames)),\n    ('alpha_extended', AlphaFeatureExtender())\n])\n\n# Pipeline for imputing metadata\nmeta_processing = Pipeline([\n    ('meta_columns', ColumnSelector(['age','BMI','gender'])),\n    ('meta_imputer', MetaDataImputer())\n])\n\n# Concatenating results \ncombined_features = FeatureUnion([\n    ('abun_part', abundance_processing),\n    ('alpha_part', alpha_processing),\n    ('meta_part', meta_processing)\n])\n\n\nlasso_param_grid = {\n    \"lasso__C\": [0.001, .01, .1, 1, 10, 100],\n}\n\ndt_param_grid = {\n    \"dt__max_depth\": [3,4,5,6],\n    \"dt__criterion\":['gini','entropy'],\n    \"dt__min_samples_split\":[2,4,6,8,10]\n}\n\nrf_param_grid = {\n    \"rf__n_estimators\":[50,100,150,200],\n    \"rf__max_depth\":[3,4,5,6],\n    \n}\n\nlasso_pipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('lasso', LogisticRegression(penalty='l1',solver='liblinear', max_iter=10000))\n])\n\ndt_pipe = Pipeline([\n    ('dt', DecisionTreeClassifier())\n])\n\nrf_pipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('rf', RandomForestClassifier())\n])\n\n\n\n\nShow the code\n\nclf, per, st = build_classifier(lasso_pipe, lasso_param_grid, train_X, train_y)\n\nprint(f' Best estimator (performance={per:.2f}):')\nprint(clf)\n\n\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n/Users/pankaj/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  warnings.warn(\n\n\n Best estimator (performance=0.90):\nPipeline(steps=[('scaler', StandardScaler()),\n                ('lasso',\n                 LogisticRegression(C=100, max_iter=1000, penalty='l1',\n                                    solver='liblinear'))])\n\n\n\n\nShow the code\nclf, per, st = build_classifier(dt_pipe, dt_param_grid, train_X, train_y)\n\nprint(f' Best dt estimator (performance={per:.2f}±{st:.2f}):')\nprint(clf)\n\n\n Best dt estimator (performance=0.83±0.09):\nPipeline(steps=[('dt',\n                 DecisionTreeClassifier(criterion='entropy', max_depth=4,\n                                        min_samples_split=10))])\n\n\n\n\nShow the code\nclf, per, st = build_classifier(rf_pipe, rf_param_grid, train_X, train_y)\n\nprint(f' Best rf estimator (performance={per:.2f}±{st:.2f}):')\nprint(clf)\n\n\n Best rf estimator (performance=0.93±0.07):\nPipeline(steps=[('scaler', StandardScaler()),\n                ('rf', RandomForestClassifier(max_depth=6, n_estimators=200))])",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  },
  {
    "objectID": "chapter6.html#target-class-distribution-across-cohorts",
    "href": "chapter6.html#target-class-distribution-across-cohorts",
    "title": "11  Building a CRC classifier",
    "section": "",
    "text": "French and Austrian cohorts are highly skewed in terms of target class distribution, both having ~ 30% cases of CRC.\nWhile, German and Italian cohorts have relatively balanced cases of malign and benign tumors.\n\n\nShow the code\nfor country in ['france','austria','germany','italy']:\n    plt.figure()\n    \n    # Get country specific dataset\n    dataset = get_country_dataset('Nine_CRC_cohorts_taxon_profiles.tsv',country,country_dataset_mapping)\n\n    # Plot class ditribution\n    #sns.countplot(data=dataset, x='target_class',alpha=.6,order=['benign','malign'],palette={'benign':'green','malign':'red'})\n    #plt.ylim([0,130])\n    #plt.show()\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n(a) French cohort\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n(b) Austrian cohort\n\n\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n(c) German cohort\n\n\n\n\n\n\n\n\n&lt;Figure size 640x480 with 0 Axes&gt;\n\n\n(d) Italian cohort\n\n\n\n\n\n\n\nFigure 11.2: Target class distribution in cohorts",
    "crumbs": [
      "Colorectal Cancer Detection",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Building a CRC classifier</span>"
    ]
  }
]